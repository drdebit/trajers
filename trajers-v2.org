#+TODO: TODO(t) IN-PROGRESS(i) | DONE(d)
#+OPTIONS: toc:nil
#+Latex_header: \usepackage{setspace}
#+latex_header: \doublespacing
#+latex_header: \usepackage[citestyle=authoryear,bibstyle=authortitle,hyperref=true,backref=true,maxcitenames=3,uniquename=false,maxbibnames=99,url=true,backend=biber,natbib=true] {biblatex}
#+latex_header: \usepackage[margin=1in]{geometry}
#+latex_header: \usepackage{array}
#+latex_header: \usepackage{amsmath}
#+latex_header: \usepackage{amssymb}
#+latex_header: \usepackage{amsthm}
#+latex_header: \newtheorem{assumption}{Assumption}
#+latex_header: \newtheorem{proposition}{Proposition}
#+latex_header: \newtheorem{definition}{Definition}
#+latex_header: \newtheorem{corollary}{Corollary}
# Hack for square brackets
#+latex_header: \newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
#+latex_header: %\ExecuteBibliographyOptions{parentracker=false}

#+latex_header: \makeatletter

#+latex_header: \newrobustcmd*{\parentexttrack}[1]{%
  #+latex_header: \begingroup
  #+latex_header: \blx@blxinit
  #+latex_header: \blx@setsfcodes
  #+latex_header: \blx@bibopenparen#1\blx@bibcloseparen
  #+latex_header: \endgroup}

#+latex_header: \AtEveryCite{%
  #+latex_header: \let\parentext=\parentexttrack%
  #+latex_header: \let\bibopenparen=\bibopenbracket%
  #+latex_header: \let\bibcloseparen=\bibclosebracket}

#+latex_header: \makeatother
#+latex_header: \addbibresource{/home/matt/Dropbox/BibLaTeX/library.bib}

# # Start document
# #+title: Sympathy for the Noise Trader
# #+author: Matthew D. DeAngelis
\begin{titlepage}
\singlespacing
\begin{center}
\LARGE Sympathy for the Noise Trader: Limitations to Learning from Price
\vspace*{35mm}

\normalsize Matthew D. DeAngelis\textsuperscript{a}

\textit{Georgia State University}
\end{center}

\vspace*{\fill}
\textsuperscript{a} Corresponding author. School of Accountancy, J. Mack Robinson College of Business, Georgia State University, 35 Broad Street, Room 512, Atlanta GA, 30302. Phone: (404) 413-7214. E-mail: mdeangelis@gsu.edu

\vspace*{10mm}
I gratefully acknowledge the contributions of Kris Allee, John Campbell, Vic Lee, Ruiyan Luo, Joao Mota, Greg Waymire, James Wilhelm, Jing Zhang and Lu Zhang.
\end{titlepage}

#+LATEX: \newpage

#+BEGIN_CENTER
\LARGE Sympathy for the Noise Trader: Limitations to Learning from Price
#+END_CENTER
\vspace*{40mm}
#+BEGIN_abstract

Empirical studies in accounting and finance assume that the market not only incorporates all available information into price, but also forms precise and efficient estimates of individual valuation parameters such as the persistence of earnings. However, prevailing models of investor learning do not provide a mechanism by which investors distinguish between multiple parameters when observing aggregate price movements. This paper extends the model in citet:grossmanImpossibilityInformationallyEfficient1980 to accommodate multiple parameters and shows that this creates an identification problem: a single price cannot uniquely determine $n$ individual parameters when $n > 1$. Formal results show that this forces investors to apply average persistence to the aggregate, generating forecast errors that increase in both the number of components and the dispersion of their persistence rates. Each supplementary signal --- a disclosure, an analyst report --- that reveals a component value reduces these errors. An agent-based simulation confirms that the market prices aggregate information efficiently while maintaining significant dispersion in individual parameter estimates. Aggregate market efficiency therefore does not imply efficient pricing of the individual components that make up that aggregate. The paper asks whether the literature ascribes too much power to price in communicating information between investors.

#+END_abstract
#+LATEX: \newpage

* Introduction
  This paper extends previous models of investor learning from price to accommodate the simultaneous processing of multiple information signals. Prior models focus on learning from price as a perfect or noisy signal for a single informational parameter (citet:grossmanImpossibilityInformationallyEfficient1980,diamondInformationAggregationNoisy1981). However, in practice information rarely arrives in isolation: detailed numerical or textual information can be released as part of an earnings announcement; managers can issue a forecast in conjunction with earnings; and an annual report can contain hundreds of pages. The analysis examines what happens when uninformed investors must revise their estimates of multiple valuation parameters from movements in a single price signal.

  The core insight is one of identification. In existing models, a single price communicates a single unknown parameter, and the system is exactly identified: an investor observing price can, in principle, recover the unknown. When price reflects multiple parameters simultaneously, however, the system becomes underidentified. A single price constrains an \(n\)-dimensional parameter vector to lie on an \(n-1\)-dimensional hyperplane, leaving infinitely many parameter combinations consistent with any observed price. This is not a problem of noise, bounded rationality, or suboptimal updating --- it is a mathematical constraint that applies regardless of investor sophistication.

  The paper derives formal propositions characterizing the identification problem and its consequences for forecasting. Following citet:grossmanImpossibilityInformationallyEfficient1980 (GS), it also develops an agent-based simulation that confirms these results computationally: the market converges on the correct aggregate price while maintaining significant dispersion in individual parameter estimates, consistent with the formal results.

  This paper makes three contributions. First, although empirical researchers routinely assume that the market ``learns'' or ``estimates'' the value of individual parameters from price, the analysis shows that the market can converge on the correct aggregate price without establishing correct consensus estimates of those parameters. Whether market price aggregates /all/ available information is distinct from whether the market learns /each component/ of information. This helps explain empirical evidence of price drift after earnings announcements (citet:bernardPostEarningsAnnouncementDriftDelayed1989,bernardEvidenceThatStock1990), earnings fixation (citet:sloanStockPricesFully1996), investor reliance on returns over fundamentals (citet:blankespoorWhyIndividualInvestors2019), and investor response to stale information (citet:tetlockAllNewsThats2011,drakeUsefulnessHistoricalAccounting2016a).

  Second, the paper shows that this identification problem --- when investors must learn from a single price signal --- is not a consequence of noise or bounded rationality but of the structure of the information environment. The mathematical principle that a lower-dimensional signal cannot uniquely encode a higher-dimensional state was established by citet:radnerRationalExpectationsEquilibrium1979, citet:allenStrictRationalExpectations1982, and citet:jordanGenericExistenceRational1982 for general equilibrium, but has not been applied to single-security investor learning. The formal results show that investors who cannot decompose the aggregate are forced to apply average persistence (Proposition \ref{prop:forecast}), the expected forecast error scales with $n\sigma^2\text{Var}_n(\boldsymbol{\alpha})$ (Proposition \ref{prop:mse}), and each supplementary signal that reveals a component collapses one dimension of the solution set (Proposition \ref{prop:collapse}).

  Third, these results suggest testable cross-sectional predictions: pricing anomalies should be larger for securities with more components and more heterogeneous persistence profiles, and market responses to disclosures should be larger for components whose persistence deviates most from the average of the remaining unknowns (Corollary \ref{cor:marginal}). Since price cannot perform the learning role that the literature assumes for it, investors likely need to rely on disclosures, analyst reports, and other signals to decompose the aggregate.

* Literature Review
  :PROPERTIES:
  :CUSTOM_ID: litreview
  :END:

** Learning from price
   :PROPERTIES:
   :CUSTOM_ID: litreview_price
   :END:
   What precisely is meant by market efficiency, as well as what and how investors learn from price, is usually not specified in empirical studies. However, a discussion in citet:tetlockGivingContentInvestor2007, regarding how the market is expected to respond to the release of media articles, is instructive as to the assumptions commonly made.

   citet:tetlockGivingContentInvestor2007 considers how the market is expected to respond under three alternative situations:
1) Media articles convey new information to the market. In this case, the market price is expected to adjust and this adjustment is expected to persist.
2) Media articles convey old information to the market. In this case, the market price is not expected to adjust, since the information is already incorporated in price.
3) Media articles reflect ``sentiment'' in the market. In this case, the market price is expected to adjust, but this adjustment is expected to reverse as arbitrageurs trade back to fundamentals.
Each alternative requires that investors not only know price, but know why price is what it is. How else would an investor distinguish between old information and new information, if she is not able to observe price and infer the information contained within it? Likewise with sentiment: an investor must understand whether private information exists and whether price incorporates that information before concluding that an observed price movement has no informational basis. A similar set of assumptions is commonly used in the accounting literature. In citet:kormendiEarningsInnovationsEarnings1987, for example: ``Implicit in [Equation] 1 is the efficient markets assumption that only the new information in earnings affects returns.'' (page 326). This suggests that the market develops an efficient estimate, not just of the total information available to value the security, but of earnings specifically. Moreover, while Kormendi and Lipe make this assumption over an entire year's time, subsequent studies use an event-study framework in which the market is presumed to fully price earnings within a few days, or even hours. In other words, investors are presumed to exit the trading process with a correct estimate of the weight to assign to earnings, at least on average, so that future price movements are determined only by earnings innovations.

  These assumptions regarding investor and market learning are based on models showing price to be a powerful information communicator, of which citet:grossmanImpossibilityInformationallyEfficient1980 (GS) is one of the most prominent. GS utilize a market model in which the return from a single risky security is based on a single informational parameter, so that return is represented as:
\begin{equation}
u = \theta + \epsilon
\end{equation}
where $\theta$ represents the mean of the return distribution. Investors can purchase $\theta$ and become informed investors. Otherwise, uninformed investors can learn $\theta$ through price movements.

  GS' main finding is that price can be too informative: it communicates the valuation parameter so well that it creates a disincentive for investors to become informed. Specifically, because the benefits of becoming informed decline as more investors become informed, communication of information through price reduces the likelihood that the marginal investor chooses to purchase information. In order for the market to reach equilibrium, GS find that market prices must convey $\theta$ with noise so that returns to information gathering are more stable.

  Price can become a sufficient statistic for unobservable information even in the absence of fully informed investors. In citet:grossmanEfficiencyCompetitiveStock1976, citet:diamondInformationAggregationNoisy1981, and citet:verrecchiaConsensusBeliefsInformation1980, investors are either endowed with or can purchase random draws from the return variable, so that no individual investor knows the true mean of returns. However, their collective information converges on the true mean, again allowing individual investors to rely solely on price and discouraging them from collecting information. As above, in order for markets to function, price must be made less informative or, as noted in Diamond and Verrecchia, ``[n]oise is critical to the analysis of the model's equilibrium'' (page 233). This noise ``represents factors other than information which cause prices to vary'' (page 234), allowing informed investors to profit from information gathering.

  Other models of noisy investor learning include citet:brayLearningEstimationStability1982 and citet:brayRationalExpectationsEquilibria1986, in which investors must learn to form rational expectations from price because the relationship between price and the security's underlying value changes over time. In these models investors use econometric models such as linear regression to estimate this relationship. citet:fourgeaudLearningProceduresConvergence1986 allow decision-makers to predict an unobservable endogenous factor from observable exogenous factors. In each of these cases, however, investors tie a single price to a single information parameter.

  As pointed out in citet:verrecchiaConsensusBeliefsInformation1980, the market can be said to have learned the distribution of value even if all investors do not agree. Instead, investors could be dispersed around the true mean on the univariate prior. The disagreement around the prior results in trading, but the same number of investors revise their beliefs upwards as downwards, in the same amounts. In this case there is trading, at least for a time, but the trading results in no price movement. In both cases, however, the market can be said to have reached both an efficient estimate of price, or the total value of the security, as well as each of the components of value (since there is only a single component).

  Models that do consider multiple information parameters combine them with multiple prices. Usually this constitutes a simulation of an entire market in which investors trade across multiple securities in an effort to achieve an efficient price equilibrium.  citet:blumeLearningBeRational1982 is a notable early contribution to this literature, along with studies such as citet:axtellComplexityExchange2005 that explore the utility of different trading mechanisms. However, the ratio of parameters to prices in these models remains one-to-one, so that each unknown is paired with its own signal. As such, these models do not extend to situations where price is determined by more than one valuation parameter: since there is a one-to-one mapping between the underlying parameter and price, it is trivial for an investor to determine whether price conveys new information and how to incorporate that information into her own estimate. If the price an investor observes is consistent with her univariate prior, she rightly believes that she has learned all that she needs to know. A media article reaffirming that belief has no effect on her trading.

  However, it is not clear that this pattern extends to a multivariate prior. If there are multiple combined values of parameters that can lead to the same price, then investors 1) cannot perfectly infer from price movements which parameter requires revision and 2) cannot perfectly infer from information about parameters what the price response should be. As a result, price no longer conveys perfect information and no longer requires the introduction of noise to prevent perfect learning.

  citet:gelsominLearningHypothesisRevisited2023 make this point forcefully in a recent discussion of the ``Learning Hypothesis'' --- the premise that managers learn decision-relevant private information by observing outsiders' trading as reflected in stock prices. They identify three key assumptions underlying this literature: (1) outsiders have private, relevant information; (2) outsiders' trading impounds the value implications of that information into prices; and (3) managers can /extract or infer/ the private information from the resulting stock prices. While the first two assumptions are well-supported, Gelsomin and Hutton note that the third is not: ``There is no empirical proxy that directly demonstrates managerial learning... managerial learning from stock prices is simply /inferred or assumed/ from the documented changes in investment sensitivity to price.'' Their metaphor is apt: predicting the shape of an ice cube from a bowl of water is easy (the forward problem), but inferring the original ice configuration from a bowl of water is impossible (the inverse problem) --- many different shapes produce the same outcome. This is precisely the structural identification problem formalized below. citet:bondRealEffectsFinancial2012 draw a related distinction between ``forecasting price efficiency'' (price is useful for predicting aggregate cash flows) and ``revelatory price efficiency'' (price is useful for informing specific real decisions). Aggregate price convergence delivers the former; it does not deliver the latter when the security's value depends on multiple parameters.

** Related work on multi-dimensional uncertainty

  The mathematical principle underlying this study --- that a lower-dimensional signal cannot uniquely encode a higher-dimensional state --- was established in the general equilibrium literature on rational expectations. citet:radnerRationalExpectationsEquilibrium1979 showed that fully-revealing rational expectations equilibria exist generically when the dimension of the price space is at least as large as the dimension of the state space. citet:allenStrictRationalExpectations1982 stated this dimensional condition explicitly, and citet:jordanGenericExistenceRational1982 proved the converse: when the dimension of private information states exceeds the dimension of the price space, fully revealing equilibria /generically do not exist/. citet:pietraTradeRevelationInformation2008 subsequently characterized the partially revealing equilibria that arise in this higher-dimensional case.

  It is important to understand precisely what these papers show and do not show. The ``prices'' in Radner and Jordan are the /vector/ of commodity prices across an entire multi-commodity economy, not the price of a single security. Their ``states'' are states of nature that determine endowments, preferences, and technologies across the economy. Their results are /existence theorems/ for general equilibrium: they ask whether there exists an equilibrium price /function/ --- a mapping from states to price vectors --- that is injective (one-to-one), so that observing the price vector uniquely identifies the underlying state. They show that such equilibria exist generically when the price vector has at least as many dimensions as the state space, and generically do not exist otherwise. They do not model investor /learning/ --- there are no dynamics, no updating rules, and no convergence process. They do not ask what happens when the condition is violated; Jordan simply shows that fully-revealing equilibria cannot generically exist in that case.

  The models that followed in the finance literature maintained the dimensional condition by construction. citet:grossmanImpossibilityInformationallyEfficient1980, citet:diamondInformationAggregationNoisy1981, and citet:verrecchiaConsensusBeliefsInformation1980 all model a single security whose value depends on a single unknown parameter, producing a one-to-one mapping that is perturbed only by noise. citet:admatiNoisyRationalExpectations1985 extends the framework to multiple assets, but with $K$ assets there are $K$ prices, preserving the one-to-one ratio. citet:blumeLearningBeRational1982 explicitly require that signals and prices be paired for Bayesian learning to function (pages 343--344).

  Yet for any single security, the dimensional condition is /trivially and always violated/. A stock's value depends on earnings persistence, growth rates, discount rates, accrual quality, and numerous other parameters --- $n$ unknowns --- but the market produces only a single price. The system is structurally underdetermined regardless of noise, agent sophistication, or market microstructure. The theoretical literature has avoided confronting this by modeling single-parameter securities (or, equivalently, by adding securities until the price vector matches the state space). The empirical literature has proceeded as if the problem does not apply, routinely assuming that investors learn individual parameters from aggregate price movements.

  This study addresses this gap. It applies the mathematical principle established by Radner and Jordan to the specific, practical context they did not consider: a single security with $n$ valuation parameters and a single price. The analysis models investor learning dynamics, demonstrates computationally what happens (aggregate convergence coexists with component-level non-identification), measures how component errors scale with $n$, and connects the result to the assumptions embedded in empirical accounting and finance research.

  Several applied papers build models in which a single price aggregates multiple unknowns, though each studies a different consequence of this structure. citet:averyMultidimensionalUncertaintyHerd1998 show that with a single dimension of uncertainty, a single price mechanism prevents herding, but with two or three dimensions of uncertainty, herding and mispricing arise precisely because a single price cannot disentangle the multiple dimensions. Their focus is on herd behavior as a consequence of this identification failure, whereas this study focuses on the limits of individual investor learning. citet:goldsteinInformationDiversityComplementarities2015 model a security whose value depends on two fundamentals, showing that different traders informed about different fundamentals create strategic complementarities in information acquisition; their model explicitly features the dimensionality mismatch (two unknowns, one price) but focuses on complementarities rather than on measuring component-level learning failure. citet:sockinInformationalFrictionsCommodity2015 study commodity producers who cannot distinguish supply from demand shocks in a single commodity price, generating feedback amplification --- a clean applied example of the 2-unknowns-from-1-signal problem. In the macroeconomics literature, citet:jonathanjadamsMacroeconomicModelsIncomplete2024 introduces a /Sufficient Idiosyncrasy Condition/ (SIC) for equilibrium uniqueness in models with endogenous signals, showing that aggregate signals such as asset prices necessarily fail this condition because aggregation zeroes out idiosyncratic variation. Adams's framework concerns the macroeconomic setting --- whether a system of aggregate signals across an economy can identify the underlying state --- rather than the single-security problem studied here. However, the underlying mathematical mechanism is the same: when a signal is a pure aggregate, it cannot be inverted to recover the individual components that produced it.

  In the accounting literature, as discussed in Section [[#litreview_price][2.1]], citet:gelsominLearningHypothesisRevisited2023 identify the absence of direct evidence that managers or investors recover individual information components from price. citet:ramananPromotingInformativenessStaggered2015 provide formal support for this concern, showing that a firm releasing multiple pieces of information simultaneously cannot learn from the market's reaction which piece the market is responding to; staggered release allows each piece to be separately priced, providing disclosure-side support for the identification problem this paper explores from the investor-learning side. The accrual anomaly literature (citet:sloanStockPricesFully1996) documents that stock prices behave as if investors fixate on total earnings without distinguishing the differential persistence of accruals and cash flows --- empirical evidence consistent with the structural identification problem, though the literature has not connected this pattern to the dimensional constraint.

  citet:godeAllocativeEfficiencyMarkets1993 show that ``zero-intelligence'' traders in a double auction can achieve near-perfect allocative efficiency through the market mechanism alone, even without any learning or information processing. Their result demonstrates that market institutions can substitute for individual rationality in producing efficient /prices/. However, Gode and Sunder's traders have fixed, known private values and do not update beliefs; their setting involves no information asymmetry and no learning from price. The agent-based simulation literature on information aggregation, including recent work by citet:jamalEmergenceInformationAggregation2024 and the comprehensive survey by citet:axtellAgentBasedModelingEconomics2025, has generally modeled fundamentals as single-dimensional. No prior study formalizes how component-level errors scale with the number of parameters for a single security, nor demonstrates this computationally. This study fills this gap.

  Several other studies examine settings where a single signal imperfectly communicates multidimensional information, though the impediment to learning is /noise/ rather than /structural underidentification/. citet:felthamPerformanceMeasureCongruity1994 show that a single performance measure cannot capture multi-dimensional agent effort in a contracting setting, and citet:fischerReportingBias2000 study how reporting bias distorts a single earnings signal. In each case, the perturbation to learning arises from exogenous noise added to a fundamentally /identified/ system: remove the noise and the signal perfectly reveals the unknown. This study identifies a different mechanism: the underidentification is /structural/, arising from the dimensionality mismatch between the parameter space ($n$ unknowns) and the signal space (a single price). Even in the complete absence of noise, a single price cannot uniquely determine the individual parameters. This distinction between /noisy identification/ and /structural non-identification/ has not been drawn in the market learning literature.

* Model

  This section formally states the model assumptions. These assumptions are implemented in an agent-based simulation described in [[#a_sim][Appendix A]], which provides executable code allowing the model to be replicated and extended.

** Formal Setup

  Consider a single risky security whose fundamental value $V$ is determined by $n$ valuation parameters:

\begin{assumption}[Security Value]
\label{a:security}
The security has a true value $V = \sum_{i=1}^{n} \theta_i$, where $\boldsymbol{\theta} = (\theta_1, \ldots, \theta_n)$ is the vector of valuation parameters. Each $\theta_i$ is drawn independently from $\mathcal{N}(0, 1)$.
\end{assumption}

\begin{assumption}[Investor Types]
\label{a:investors}
There are $N$ investors indexed by $j = 1, \ldots, N$. A fraction $\lambda \in (0,1)$ are \emph{informed} and the remainder are \emph{uninformed}. The informed/uninformed status of each investor is fixed throughout the trading process.
\end{assumption}

\begin{assumption}[Information Endowments]
\label{a:information}
Each informed investor $j \in \mathcal{I}$ knows the true parameter vector: $\hat{\boldsymbol{\theta}}^j = \boldsymbol{\theta}$. Each uninformed investor $j \in \mathcal{U}$ holds a prior $\hat{\boldsymbol{\theta}}^j = (\hat{\theta}_1^j, \ldots, \hat{\theta}_n^j)$, where each $\hat{\theta}_i^j$ is drawn independently from $\mathcal{N}(0, 1)$.
\end{assumption}

The informed fraction is set exogenously rather than derived from an information acquisition decision as in GS. The focus is what happens /after/ some investors have become informed --- specifically, how well uninformed investors can learn individual parameters from the price signal generated by trading. Setting $\lambda$ exogenously is equivalent to examining GS at a particular equilibrium point and asking what uninformed investors can learn at that point. Uninformed investors' priors are drawn from $\mathcal{N}(0,1)$, the same distribution that generates the true parameters, so priors are calibrated to the distribution of fundamentals even though individual investors' priors will generally not equal the true values.

\begin{assumption}[Market Price]
\label{a:price}
The market maintains a single scalar price $p_t$ at each time $t$. The initial price is $p_0 = V + \eta$, where $\eta \sim \mathcal{N}(0,1)$, so that price starts near but not at the true value. The price is updated according to a simple order imbalance rule:
\begin{equation}
p_{t+1} = p_t + \delta \cdot \text{sgn}\left(\sum_{j=1}^{N} d_j^t \right)
\end{equation}
where $\delta = 0.01$ is the tick size and $d_j^t \in \{-1, 0, 1\}$ is investor $j$'s order at time $t$.
\end{assumption}

Note the absence of noise traders. In GS, noise traders are required to prevent price from perfectly revealing $\theta$, which would eliminate the incentive to become informed. In the multi-parameter setting, noise traders are unnecessary: the identification problem itself prevents perfect revelation of individual parameters. In the single-parameter case, the model /does/ achieve perfect convergence (consistent with GS without noise traders), which serves as the benchmark for demonstrating that the multi-parameter result is driven by dimensionality, not by noise.

\begin{assumption}[Trading Rule]
\label{a:trading}
Each investor submits a unit order based on the comparison of her aggregate prior to the current price:
\begin{equation}
d_j^t = \begin{cases} 1 & \text{if } \sum_{i=1}^{n} \hat{\theta}_i^j > p_t \\ -1 & \text{if } \sum_{i=1}^{n} \hat{\theta}_i^j < p_t \\ 0 & \text{if } \sum_{i=1}^{n} \hat{\theta}_i^j = p_t \end{cases}
\end{equation}
\end{assumption}

The model adopts a simple unit-demand trading rule rather than the continuous demand functions in GS, which depend on risk aversion parameters. The identification problem does not depend on demand elasticity: whether an investor trades 1 share or 100, the price signal contains only aggregate information. Because informed investors trade on $\sum_i \theta_i = V$ and constitute a majority ($\lambda > 0.5$), the order imbalance will tend to push $p_t$ toward $V$, ensuring price convergence.

\begin{assumption}[Belief Updating]
\label{a:updating}
Informed investors do not update. Uninformed investors update their prior at time $t$ if price fails to move toward their prior (indicating that their prior is inconsistent with the majority of investors). When an uninformed investor $j$ updates, she:
1. Selects a parameter index $k$ uniformly at random from her uninformed parameters.
2. Computes the inferred value of $\theta_k$ from price, holding her other parameter estimates fixed:
\begin{equation}
\tilde{\theta}_k^j = p_t - \sum_{i \neq k} \hat{\theta}_i^j
\end{equation}
3. Updates her estimate by averaging with the inferred value:
\begin{equation}
\hat{\theta}_k^{j,\text{new}} = \frac{\hat{\theta}_k^j + \tilde{\theta}_k^j}{2}
\end{equation}
\end{assumption}

The choice of averaging as an updating rule is deliberately simple. The results do not depend on this specific rule. The core result --- that individual parameter estimates do not converge to their true values --- follows from the structure of the information environment, as shown next.

** The Identification Problem

The following proposition characterizes the fundamental limitation of learning individual parameters from an aggregate price signal.

\begin{proposition}[Structural Non-Identification]
\label{prop:nonid}
Let $V = \sum_{i=1}^{n} \theta_i$ and suppose that the market price $p$ converges to $V$. For $n = 1$, observing $p = V$ uniquely determines $\theta_1 = V$. For $n > 1$, the set of parameter vectors consistent with the observed price is
\begin{equation}
\mathcal{S}(p) = \left\{ \boldsymbol{\theta}' \in \mathbb{R}^n : \sum_{i=1}^{n} \theta_i' = p \right\}
\end{equation}
which is an $(n-1)$-dimensional hyperplane in $\mathbb{R}^n$. In particular, $|\mathcal{S}(p)| = \infty$ for $n > 1$.
\end{proposition}

This result is immediate from the structure of the problem. The key implication is that no updating rule --- whether averaging, OLS, or full Bayesian inference --- can uniquely recover the parameter vector $\boldsymbol{\theta}$ from the scalar price $p$ when $n > 1$. The system has $n$ unknowns and one equation, leaving $n - 1$ degrees of freedom. This is not a matter of noise or bounded rationality; it is a constraint on the information content of the signal itself. Note that this condition is trivially violated for any security whose value depends on more than one parameter --- which is to say, for any real security.

As discussed in Section [[#litreview][2]], the mathematical principle that a lower-dimensional signal space cannot encode a higher-dimensional state space was established by citet:radnerRationalExpectationsEquilibrium1979 and citet:jordanGenericExistenceRational1982, but their results concern multi-commodity economies rather than investor learning from a single security's price. Proposition \ref{prop:nonid} applies this principle to the single-security setting, and the simulation results below demonstrate what investors' parameter estimates actually look like when they attempt to learn from an aggregate signal that cannot identify the individual components.

This identification problem is qualitatively different from the impediments to learning studied in the noisy rational expectations literature. In GS, noise traders perturb the price signal, but the underlying system is identified: removing the noise would allow perfect recovery of $\theta$. In this setting, even in the complete absence of noise --- even if price converges exactly to $V$ --- the individual parameters remain unrecoverable from price alone. The ``noise'' in this model is endogenous, arising from the dimensionality mismatch between the information space and the signal space.

** Forecasting Under Non-Identification

Proposition \ref{prop:nonid} shows that investors cannot recover individual parameters from price. A natural question is whether this matters: if the aggregate price is correct, what is lost by not knowing the components? The answer depends on whether the components evolve differently over time. In practice, they do: cash flows persist at higher rates than accruals, growth rates differ from discount rates, and some components are transitory while others are permanent. The next subsection formalizes the consequences of non-identification for forecasting future values.

\begin{definition}[Differential Persistence]
\label{def:persistence}
Each valuation parameter $\theta_i$ evolves over time according to a persistence rate $\alpha_i \in (0,1]$. The security's value one period ahead is:
\begin{equation}
V_{t+1} = \sum_{i=1}^{n} \alpha_i \theta_{i,t}
\end{equation}
The persistence rates are heterogeneous: $\alpha_i \neq \alpha_j$ for at least some $i \neq j$. The cross-sectional variance of persistence rates is $\text{Var}_n(\boldsymbol{\alpha}) = \frac{1}{n}\sum_{i=1}^{n}(\alpha_i - \bar{\alpha})^2$, where $\bar{\alpha} = \frac{1}{n}\sum_{i=1}^{n}\alpha_i$.
\end{definition}

\begin{proposition}[Forced Average Persistence]
\label{prop:forecast}
Suppose the valuation parameters $\theta_1, \ldots, \theta_n$ are drawn independently from $\mathcal{N}(0, \sigma^2)$. An investor who observes only the aggregate $V_t = \sum_{i=1}^{n}\theta_i$ and knows the persistence vector $\boldsymbol{\alpha}$ but not the individual $\theta_i$ has as her optimal forecast of $V_{t+1}$:
\begin{equation}
\hat{V}_{t+1} = \bar{\alpha} \cdot V_t
\end{equation}
Her forecast error is:
\begin{equation}
\label{eq:ferror}
V_{t+1} - \hat{V}_{t+1} = \sum_{i=1}^{n}(\alpha_i - \bar{\alpha})\,\theta_i
\end{equation}
\end{proposition}

\begin{proof}
Given $V_t = \sum \theta_i$ and the independence and identical distribution of the $\theta_i$, the conditional expectation of each component given the aggregate is $E[\theta_i \mid V_t] = V_t / n$ by symmetry. The optimal forecast is therefore:
\begin{equation}
E[V_{t+1} \mid V_t] = \sum_{i=1}^{n} \alpha_i \cdot E[\theta_i \mid V_t] = \sum_{i=1}^{n} \alpha_i \cdot \frac{V_t}{n} = \bar{\alpha} \cdot V_t
\end{equation}
The forecast error follows by subtraction: $\sum \alpha_i \theta_i - \bar{\alpha} \sum \theta_i = \sum(\alpha_i - \bar{\alpha})\theta_i$.
\end{proof}

The key feature of Equation \eqref{eq:ferror} is that the forecast error is /not/ random noise: it is a deterministic function of the true parameter values, weighted by the deviation of each component's persistence from the average. An investor who knew the individual $\theta_i$ would incur zero forecast error. The error arises entirely from the inability to decompose the aggregate, which forces the investor to apply a single average persistence rate to what is in fact a heterogeneous vector.

\begin{proposition}[Expected Squared Forecast Error]
\label{prop:mse}
Under the conditions of Proposition \ref{prop:forecast}, the expected squared forecast error is:
\begin{equation}
\label{eq:msfe}
E\left[(V_{t+1} - \hat{V}_{t+1})^2\right] = n\,\sigma^2 \cdot \text{Var}_n(\boldsymbol{\alpha})
\end{equation}
This is increasing in both the number of components $n$ and the cross-sectional dispersion of persistence rates $\text{Var}_n(\boldsymbol{\alpha})$. The expected squared forecast error is zero if and only if all persistence rates are equal ($\alpha_i = \bar{\alpha}$ for all $i$).
\end{proposition}

\begin{proof}
Since the $\theta_i$ are independent with variance $\sigma^2$:
\begin{equation}
E\left[\left(\sum_{i=1}^{n}(\alpha_i - \bar{\alpha})\theta_i\right)^2\right] = \sum_{i=1}^{n}(\alpha_i - \bar{\alpha})^2 \cdot E[\theta_i^2] = \sigma^2 \sum_{i=1}^{n}(\alpha_i - \bar{\alpha})^2 = n\,\sigma^2 \cdot \text{Var}_n(\boldsymbol{\alpha})
\end{equation}
The cross terms vanish by independence. The expression is zero if and only if $\alpha_i = \bar{\alpha}$ for all $i$, which requires all persistence rates to be identical.
\end{proof}

Proposition \ref{prop:mse} provides two testable cross-sectional predictions. First, securities with more valuation components (larger $n$) should exhibit larger systematic forecast errors and, consequently, larger pricing anomalies. Second, holding $n$ fixed, securities whose components have more heterogeneous persistence rates should exhibit larger anomalies. The multi-component version of this prediction, involving the /dispersion/ of persistence rates across $n > 2$ components, has not been tested. Section [[#discussion][5]] discusses the empirical evidence for these predictions.

The next proposition characterizes how supplementary signals reduce forecast error by collapsing the dimensions of the identification problem.

\begin{proposition}[Dimensional Collapse]
\label{prop:collapse}
Suppose an investor observes the aggregate price $V_t$ and additionally observes $k$ component values $\{\theta_j : j \in K\}$, where $K \subset \{1, \ldots, n\}$ with $|K| = k$.

(a) The set of parameter vectors consistent with all observations is an $(n - 1 - k)$-dimensional affine subspace. When $k \geq n - 1$, the parameter vector is uniquely determined.

(b) The investor's optimal forecast is:
\begin{equation}
\hat{V}_{t+1} = \sum_{i \in K} \alpha_i \theta_i \;+\; \bar{\alpha}_{-K} \cdot \left(V_t - \sum_{i \in K}\theta_i\right)
\end{equation}
where $\bar{\alpha}_{-K} = \frac{1}{n-k}\sum_{i \notin K}\alpha_i$ is the average persistence of the unknown components.

(c) The expected squared forecast error reduces to:
\begin{equation}
\label{eq:collapse}
E\left[(V_{t+1} - \hat{V}_{t+1})^2\right] = (n - k)\,\sigma^2 \cdot \text{Var}_{n-k}(\boldsymbol{\alpha}_{-K})
\end{equation}
where $\text{Var}_{n-k}(\boldsymbol{\alpha}_{-K})$ is the cross-sectional variance of persistence rates among the $n - k$ unobserved components.

(d) When $k = n - 1$, one unknown component remains, which is determined by $\theta_{\text{last}} = V_t - \sum_{i \in K}\theta_i$. The forecast error is zero.
\end{proposition}

\begin{proof}
(a) The investor has $k + 1$ linear constraints on $n$ unknowns: the aggregate constraint $\sum \theta_i = V_t$ and the $k$ component observations $\theta_j = c_j$ for $j \in K$. These are linearly independent since each component observation involves a different variable. The solution set has dimension $n - (k + 1) = n - 1 - k$. When $k \geq n - 1$, the system has at least $n$ independent constraints, yielding a unique solution.

(b) For the known components, the investor forecasts exactly: $\sum_{i \in K}\alpha_i\theta_i$. For the unknown components, she knows only their aggregate $V_{-K} = V_t - \sum_{i \in K}\theta_i$, and by the same symmetry argument as in Proposition \ref{prop:forecast}, her optimal forecast of $\sum_{i \notin K}\alpha_i\theta_i$ is $\bar{\alpha}_{-K} \cdot V_{-K}$.

(c) The forecast error comes only from the unknown components: $\sum_{i \notin K}(\alpha_i - \bar{\alpha}_{-K})\theta_i$. The expected squared error follows as in Proposition \ref{prop:mse}, with $n$ replaced by $n - k$ and $\boldsymbol{\alpha}$ restricted to the unobserved components.

(d) When $k = n - 1$, $n - k = 1$ and $\text{Var}_1(\cdot) = 0$, so the forecast error is zero.
\end{proof}

Proposition \ref{prop:collapse} formalizes the value of supplementary signals. Each component observation eliminates one dimension of the solution set and reduces the forecast error. Equation \eqref{eq:collapse} shows that the error reduction depends on /which/ component is revealed: observing a component whose persistence deviates greatly from the remaining average produces a larger error reduction than observing one close to the average. The following corollary makes this precise.

\begin{corollary}[Marginal Value of a Signal]
\label{cor:marginal}
Given $k$ observed components, the reduction in expected squared forecast error from additionally observing component $\ell \notin K$ is:
\begin{equation}
\Delta \text{MSFE} = (n - k)\,\sigma^2\,\text{Var}_{n-k}(\boldsymbol{\alpha}_{-K}) \;-\; (n - k - 1)\,\sigma^2\,\text{Var}_{n-k-1}(\boldsymbol{\alpha}_{-(K \cup \{\ell\})})
\end{equation}
This reduction is largest when $\alpha_\ell$ is the persistence rate most distant from $\bar{\alpha}_{-K}$ among the unobserved components.
\end{corollary}

This corollary has a direct empirical implication: the most valuable disclosure is the one that addresses the component whose persistence rate is most different from the average of the remaining unknown components. In the context of citet:sloanStockPricesFully1996, if cash flow persistence ($\alpha_{\text{CF}}$, high) deviates more from the overall average than accrual persistence ($\alpha_{\text{ACC}}$, low), then disclosing cash flows is more valuable than disclosing accruals --- and vice versa, depending on the persistence profile of the other unknown components. More generally, the corollary predicts that the market response to a disclosure should be larger for components with more extreme persistence, because those are the components that resolve more forecast uncertainty.

* Results
  Having established the formal properties of the identification problem, this section tests whether they manifest in a computational setting. Propositions \ref{prop:nonid}--\ref{prop:mse} predict that (1) a single price converges to the correct aggregate value, (2) individual parameter estimates remain dispersed along the solution hyperplane $\mathcal{S}(p)$, and (3) forecast errors increase with the number of components. The agent-based simulation confirms predictions (1) and (2) directly with $N = 500$ investors ($\lambda = 0.6$ informed, $0.4$ uninformed) per market, and parameter-estimation errors increase with $n$, consistent with the dimensionality argument underlying Proposition \ref{prop:nonid}. Full implementation details and source code are provided in [[#a_sim][Appendix A]].

  The analysis first examines the single-parameter case ($n = 1$), which serves as a benchmark. In this case, the system is exactly identified: there is one unknown ($\theta_1$) and one signal ($p$). Consistent with GS, both the market price and uninformed investors' priors converge to the true value $V = \theta_1$, as shown in Figure [[fig:1-param-price]]. Price convergence implies parameter convergence, as Proposition \ref{prop:nonid} predicts for $n = 1$. Markets converge in roughly 87 trading rounds on average.

#+BEGIN_CENTER
[Figure [[fig:1-param-price]] here]
#+END_CENTER

  The simulation then extends to a two-parameter security ($n = 2$). As shown in Figure [[fig:2-param-price]], price again converges to the correct aggregate value $V = \theta_1 + \theta_2$, and uninformed investors correctly update their aggregate priors. The market prices all available information about the security in aggregate.

#+BEGIN_CENTER
[Figure [[fig:2-param-price]] here]
#+END_CENTER

  However, individual parameter estimates tell a different story. Figure [[fig:2-param-investors]] plots each investor's prior as the market converges, with the x-axis showing $\hat{\theta}_1$ and the y-axis $\hat{\theta}_2$. Uninformed investors converge only on the /sum/ of their prior, producing a fitted line that passes through the true aggregate value while maintaining diffusion on the individual parameters. This is precisely the hyperplane $\mathcal{S}(p)$ described in Proposition \ref{prop:nonid}: in two dimensions, it is a line $\hat{\theta}_1 + \hat{\theta}_2 = V$. Since price only communicates the sum of the parameters, any prior that produces that sum is perceived as equally valid.

#+BEGIN_CENTER
[Figure [[fig:2-param-investors]] here]
#+END_CENTER

  The consensus estimates illustrate the identification problem concretely. In a representative two-parameter market with true values $(\theta_1, \theta_2) = (0.26, -0.20)$, the average estimates across all investors were approximately $(0.19, -0.13)$. The errors on individual parameters are approximately equal in magnitude but opposite in sign: the overestimation of $\theta_1$ is compensated by an underestimation of $\theta_2$, so that the total summed error is roughly zero. This is a direct consequence of the identification constraint: any parameter vector on the hyperplane $\hat{\theta}_1 + \hat{\theta}_2 = V$ produces the correct aggregate price, so errors in individual parameters must net to zero. Although price serves as an aggregator for imperfect individual information sets, the aggregate of investors' beliefs does not reflect the true value of the individual parameters.

  To examine how parameter errors scale with dimensionality, the analysis varies the number of parameters from 2 to 10, generating 500 markets for each value. For each market, the mean squared error of the consensus parameter estimates is computed:

\begin{equation}
\text{MSE}(n) = \frac{1}{500} \sum_{m=1}^{500} \frac{1}{n} \sum_{i=1}^{n} \left(\theta_i^m - \bar{\hat{\theta}}_i^m \right)^2
\end{equation}

where $\theta_i^m$ is the true value of parameter $i$ in market $m$ and $\bar{\hat{\theta}}_i^m$ is the average estimate across all investors.

  As shown in Table [[tbl:sse-errors]], the mean squared error is systematically higher than the two-parameter baseline for all values of $n$ tested, though the increase is not strictly monotonic across adjacent values. The increase is most dramatic from two to three parameters, with diminishing marginal increases thereafter. All comparisons to the two-parameter case are statistically significant at conventional levels, confirming that increasing the number of parameters increases the mispricing of individual parameters. This is consistent with Proposition \ref{prop:nonid}: as $n$ grows, the hyperplane $\mathcal{S}(p)$ gains dimensions, and the space of parameter vectors consistent with the observed price expands. The pattern is also qualitatively consistent with the scaling predicted by Proposition \ref{prop:mse} for forecast errors: each additional parameter adds a dimension that the single price signal cannot resolve, increasing both parameter-estimation error (measured here) and forecast error (derived analytically in Proposition \ref{prop:mse}).

#+BEGIN_CENTER
[Table [[tbl:sse-errors]] here]
#+END_CENTER

* Discussion of Results
  :PROPERTIES:
  :CUSTOM_ID: discussion
  :END:
  The results suggest that the market can converge on the correct value of a security without establishing a correct estimate for each valuation parameter. As a result, even after market price has converged on the correct value, investors can still be ``surprised'' by information that was previously available to informed investors. This insight has implications for interpreting market responses. The discussion first applies it to several empirical phenomena --- investor learning from stale information, post-earnings-announcement drift, and investor reliance on aggregate signals --- then examines how it relates to competing explanations and develops the implications of supplementary signals for forecasting and disclosure.

** Stale information and investor learning

  According to the framework in citet:tetlockGivingContentInvestor2007, a researcher can distinguish between media articles containing old (previously released and priced) information and media articles reflecting or creating investor sentiment by their price responses: the first does not move prices, while the second results in a temporary price movement followed by a reversal.

  The identification problem complicates this distinction. Consider a company that issues an earnings release containing both earnings and another piece of value-relevant information, resulting in a price movement that reflects both components simultaneously. A journalist subsequently writes an opinion piece articulating the true value that should be assigned to earnings. Under the standard efficient markets hypothesis, this information is stale and should not move prices. However, because investors could only imperfectly learn this component from the aggregate price signal, the value reported by the journalist will differ from the consensus value that investors have assigned to earnings. Those investors who read the article believe they are observing new information and revise their estimates accordingly.

  As a result, these investors trade on what they perceive as new information, affecting price. Informed investors who already knew the information observe the resulting mispricing and trade back to fundamental value. Uninformed investors then interpret this return to fundamental value as information about the second parameter, and as they learn it from price, the market achieves correct valuation of each component as well.

  This analysis, then, suggests that price reactions to seemingly ``stale'' information can reflect genuine investor learning rather than noise. citet:tetlockAllNewsThats2011 provides direct evidence: using over 350,000 news stories, he finds that stock returns respond to stale news (stories textually similar to recent coverage), and that individual investors trade /more aggressively/ on stale news than on fresh news. Under the standard view, this is anomalous. But if investors cannot determine from price alone which components are already priced, it makes sense that they would respond to any signal --- even a recycled one --- that helps them figure out how to attribute portions of the aggregate to specific fundamentals. Consistent with this interpretation, citet:drakeUsefulnessHistoricalAccounting2016a document that investors actively access historical filings around new disclosure events, suggesting they use stale reports to provide context for decomposing the information in new disclosures.

  Whether these temporary fluctuations in price are considered impairments to market efficiency depends on whether the objective of markets is to achieve and maintain a perfectly correct price, even if market participants do not fully understand how that price is determined, or to communicate information between investors.

** Post-earnings-announcement drift and earnings fixation

  The identification problem also suggests a different perspective on post-earnings-announcement drift (PEAD), one of the most robust anomalies in finance. citet:bernardPostEarningsAnnouncementDriftDelayed1989 documented that after earnings are announced, cumulative abnormal returns continue to drift in the direction of the earnings surprise for 60 or more trading days. The standard interpretation is that investors underreact to earnings news --- they fail to fully process the implications of current earnings for future earnings.

  The identification problem suggests a different mechanism. Consider an investor observing an earnings surprise. To respond optimally, she must determine what portion of the surprise represents genuinely new information versus what was already reflected in the pre-announcement price. But the pre-announcement price is an aggregate signal that conflates all valuation parameters. The investor cannot decompose it to verify which components were already priced. Faced with this uncertainty, a reasonable response is to revise gradually: incorporate some of the surprise immediately, then wait for additional signals --- subsequent disclosures, analyst reports, trading by informed investors --- to help sort out the decomposition. This gradual revision produces exactly the drift pattern that Bernard and Thomas document.

  If this interpretation is correct, PEAD may not reflect cognitive underreaction or processing failure. Instead, it may reflect investors responding rationally to the fact that they cannot fully assess the information content of an earnings surprise relative to what was already in price. The drift occurs as supplementary signals arrive and help investors work out which components of value were already priced and which were not.

  Relatedly, citet:sloanStockPricesFully1996 documents that stock prices behave as if investors fixate on total earnings without distinguishing between the differential persistence of accruals and cash flows. The ``earnings fixation hypothesis'' holds that investors treat aggregate earnings as a sufficient statistic rather than decomposing it into components with different implications for future value. This is consistent with what non-identification predicts: when the aggregate signal is all that price conveys, it is not clear that investors have a mechanism to recover the individual components from price alone. Treating the aggregate as a sufficient statistic may be the best they can do.

  The identification problem may also help explain why earnings response coefficients are smaller than expected (citet:kothariCapitalMarketsResearch2001). Since earnings are seldom released in isolation, market participants may be unable to determine from the resulting price movement how much to attribute to earnings specifically versus other simultaneously released information.

** Investor reliance on aggregate signals

  citet:blankespoorWhyIndividualInvestors2019 exploit the introduction of automated (``robo-journalism'') earnings articles that present both earnings news and trailing stock returns side by side. Despite having earnings information delivered directly to them, individual investors ignore the earnings content and trade based on trailing stock returns. Blankespoor et al. rule out awareness and acquisition costs and attribute this behavior to ``integration costs'' --- the cognitive difficulty of translating component accounting information into a trading decision.

  This paper offers a different explanation. If investors cannot decompose the pre-existing price signal into its component parameters, then they cannot determine how a specific piece of component information (e.g., an earnings number) maps to value relative to what is already priced. Returns, by contrast, /are/ the aggregate signal --- and in an underdetermined system, the aggregate is the only signal whose information content is unambiguous. The ``integration cost'' that Blankespoor et al. identify may not be purely cognitive; it may reflect the fact that it is genuinely impossible to map individual accounting components back to their value implications when the security's value depends on multiple parameters. If so, trading on returns is the rational response.

  This pattern extends beyond individual investors. citet:hongUnifiedTheoryUnderreaction1999 model a market in which ``newswatchers'' observe private fundamental signals but cannot extract other newswatchers' information from prices. This inability to invert the price signal is the same identification problem described here. In their model, information diffuses gradually because agents cannot fully decompose the aggregate, producing initial underreaction that momentum traders exploit.

  The identification problem may also be amplified when multiple pieces of information compete for investors' attention. citet:hirshleiferLimitedAttentionInformation2003 document delayed pricing in the presence of a large number of competing firm disclosures, attributing this to limited attention. The identification problem suggests a complementary mechanism: when multiple disclosures are released simultaneously, the aggregate price response becomes even harder to decompose, making the price signal less useful regardless of how much attention investors devote to it.

** Competing explanations

  The empirical patterns described above --- drift, earnings fixation, return-chasing, and delayed pricing --- have been attributed to a variety of mechanisms. The dominant explanations include information processing costs (investors find complex firms harder to analyze, so prices adjust more slowly), gradual information diffusion (firm-specific news reaches the investing public slowly), behavioral fixation (investors fixate on aggregate numbers without decomposing them into components), and additional signals that provide independent information channels. These explanations share a common premise: that price is, in principle, an effective channel for learning about individual valuation components, and that remaining frictions determine how well investors actually price securities. The results here suggest a different premise.

  Consider the processing costs literature. citet:barinovFirmComplexityPostearnings2024 document that PEAD is substantially larger for conglomerates and for firms that organically create new business segments, attributing this to the higher cost of becoming informed about multi-segment firms. citet:cohenComplicatedFirms2012 show that returns of standalone firms in a conglomerate's industries predict the conglomerate's future returns, generating a trading strategy with returns of 118 basis points per month. Both studies invoke the Grossman-Stiglitz tradition: complexity makes learning expensive, but not in principle impossible. However, a conglomerate operating in $n$ industries has $n$ unknown fundamentals but still only one stock price; the system is underidentified regardless of analyst effort.

  citet:shiCanEarningsFixation2012 show that the accrual anomaly is strongest where both the earnings response coefficient and the persistence gap between cash flows and accruals are large --- notably, this is the same interaction that Proposition \ref{prop:mse} predicts, though Shi attributes it to behavioral fixation rather than to an identification constraint. citet:mohanramAnalystsCashFlow2014 shows that analyst cash flow forecasts decompose earnings into components and are associated with the decline of the accrual anomaly, while citet:ettredgeImpactSFASNo2005 show that SFAS 131 segment disclosures improved the market's ability to anticipate future earnings. These findings are consistent with Proposition \ref{prop:collapse}: supplementary signals that decompose the aggregate into components help resolve the identification problem.

  A particularly instructive finding comes from citet:parkEffectSFAS1312011, who decomposes future earnings into industry-wide and firm-specific components. SFAS 131 segment disclosures improved the market's ability to price the industry-wide component but /not/ the firm-specific component. This asymmetry is difficult to explain if the problem is one of processing costs or attention, which would predict uniform improvement. Under this framework, it is expected: segment disclosure reveals /which industries/ a firm operates in but does not resolve firm-specific dynamics, which require firm-specific signals. Each disclosure resolves the dimensions it directly addresses while leaving others intact.

  In each case, the existing explanations treat the pricing problem as /difficult but in principle solvable/: with better analysts, more attention, or additional disclosures, investors could in principle price individual components correctly. The results here suggest something different. The problem is not just difficult --- as Proposition \ref{prop:nonid} shows, it is structurally impossible from a single price signal when $n > 1$. This analysis does not invalidate the existing literature. Investors /do/ face processing costs, /are/ subject to limited attention, and /do/ benefit from additional signals. But these factors operate within a structural constraint that the literature has not identified: processing costs and attention determine how efficiently investors use available signals, while the identification constraint bounds what those signals can reveal.

** The value of supplementary signals: why components matter for predicting future prices

  Propositions \ref{prop:forecast}--\ref{prop:collapse} show that the identification problem creates forecast errors whose magnitude depends on the number of components and the dispersion of their persistence rates, and that supplementary signals reduce these errors. The following example illustrates these results and connects them to empirical phenomena.

  The key point is that the trading value of component information lies not in knowing today's price better --- it is already correct in aggregate --- but in predicting how today's value will evolve into tomorrow's price. An investor who observes only the aggregate is forced to apply average persistence $\bar{\alpha}$ to the sum, incurring a forecast error of $\sum(\alpha_i - \bar{\alpha})\theta_i$ (Proposition \ref{prop:forecast}). An investor who can decompose the aggregate applies the correct persistence to each component and incurs zero forecast error.

  A simple example illustrates the mechanism. Suppose a security's value depends on two components: an earnings component $\theta_1$ with persistence $\alpha_1 = 0.9$ and a transitory component $\theta_2$ with persistence $\alpha_2 = 0.3$. Today's true values are $\theta_1 = 8$ and $\theta_2 = 2$, so $V = 10$. The market price has converged to 10 --- the aggregate is correct.

  An uninformed investor who observes only $V = 10$ cannot determine how much of the value derives from the persistent component and how much from the transitory one. She might believe $(\theta_1, \theta_2) = (6, 4)$ or $(5, 5)$ or any other point on the line $\theta_1 + \theta_2 = 10$. Each of these decompositions implies a different future value:

  | Believed $(\theta_1, \theta_2)$ | Implied $V_{\text{tomorrow}}$ |
  |------|-----------|
  | $(9, 1)$ | $0.9(9) + 0.3(1) = 8.4$ |
  | $(8, 2)$ | $0.9(8) + 0.3(2) = 7.8$ |
  | $(5, 5)$ | $0.9(5) + 0.3(5) = 6.0$ |
  | $(3, 7)$ | $0.9(3) + 0.3(7) = 4.8$ |

  All four investors agree on today's price ($V = 10$), but their forecasts of tomorrow's price range from 4.8 to 8.4. The correct forecast is 7.8, but only an investor who knows the true decomposition can compute it. The range of possible future values --- from $\alpha_2 \cdot V = 3.0$ (if the entire value is transitory) to $\alpha_1 \cdot V = 9.0$ (if the entire value is persistent) --- is the direct consequence of non-identification. Each point on the solution hyperplane maps to a different $V_{\text{tomorrow}}$.

  Now suppose an earnings release reveals $\theta_1 = 8$. The investor who observes this can compute $\theta_2 = V - \theta_1 = 2$ and forecast $V_{\text{tomorrow}} = 0.9(8) + 0.3(2) = 7.8$ exactly. This is Proposition \ref{prop:collapse} at work: the supplementary signal has collapsed the solution set from a line (one-dimensional) to a point (zero-dimensional), eliminating all forecast ambiguity. The trading profit comes from the difference between the informed investor's forecast (7.8) and the market's forecast, which is based on whatever average persistence the uninformed investors implicitly apply to the aggregate.

  As Proposition \ref{prop:collapse} shows, this extends naturally to $n$ components. With $n$ components and only the aggregate price observed, the expected squared forecast error is $n\sigma^2 \text{Var}_n(\boldsymbol{\alpha})$ (Proposition \ref{prop:mse}). Each supplementary signal that pins down one component reduces the residual error to $(n-k)\sigma^2 \text{Var}_{n-k}(\boldsymbol{\alpha}_{-K})$. By Corollary \ref{cor:marginal}, the greatest error reduction comes from revealing the component whose persistence deviates most from the average of the remaining unknowns.

  This is the mechanism underlying the accrual anomaly documented by citet:sloanStockPricesFully1996. The market observes aggregate earnings $E = \text{CF} + \text{ACC}$ and prices the security correctly given that aggregate. But cash flows ($\alpha_{\text{CF}}$ high) are more persistent than accruals ($\alpha_{\text{ACC}}$ low), so a firm with high accruals and low cash flows has a current price that is too high relative to its /future/ value. An investor who can decompose $E$ into cash flows and accruals can predict this reversion and trade profitably. The market, unable to decompose the aggregate, implicitly forecasts with $\bar{\alpha}$ and is systematically wrong about future values.

  The same logic explains the profitability of trading on post-earnings-announcement drift. An earnings surprise identifies a change in a specific component ($\Delta\theta_{\text{earnings}}$). An investor who observes the surprise directly knows /which/ component changed and can predict how the security's value will evolve given the persistence of that component. The market, observing only the aggregate price change, cannot attribute it to a specific component and therefore adjusts gradually as supplementary signals arrive. The drift represents the period during which the market is resolving the decomposition problem, and informed investors can trade profitably during that window.

  More generally, the system is fully determined only when $n - 1$ components are independently observed (Proposition \ref{prop:collapse}(d)) --- a condition that is never met in practice, but that disclosure, analyst reports, and other information channels can /approximate/. The practical implication is that each additional signal provides diminishing but positive marginal value, and the most valuable signals are those addressing the components with the most extreme persistence rates.

  However, direct component observations are not the only signals that help resolve the identification problem. This paper identifies three types of information that constrain the solution set:

  /Component observations/ directly pin down a parameter value: $\theta_i = c$. An earnings disclosure, an analyst's cash flow estimate, or a segment report each provides this type of signal. In the linear algebra of the problem, a component observation eliminates one unknown, collapsing the solution set by one dimension.

  /Structural observations/ reveal not a component value but the /mapping/ from components to price. The earnings response coefficient (ERC) is a canonical example. When an investor observes both an earnings surprise $\Delta\theta_{\text{earnings}}$ and the resulting price response $\Delta V$, the ratio $\Delta V / \Delta\theta_{\text{earnings}}$ provides information about the partial derivative $\partial V / \partial \theta_{\text{earnings}}$ --- how much of a unit change in earnings translates into price. If the ERC is less than one, it indicates that other components moved in the opposite direction, or that the market had already partially incorporated the earnings news, or that earnings have implications for other components (a strong quarter may revise downward an implicit risk premium).

  These structural coefficients constrain the /shape/ of the solution set: they do not pin down a specific $\theta_i$, but they restrict how the components relate to the aggregate, narrowing the range of plausible decompositions.

  /Relative observations/ arise from comparing price responses across events. If an investor observes that the market responded strongly to an earnings surprise but weakly to a revenue surprise, she can infer something about which components the market had already priced and which dimensions remain unresolved. Each event-study observation, in this framework, is informative not just about the news content of the event but about the /residual structure/ of the identification problem --- which dimensions of the solution set have already been collapsed by prior signals.

  The empirical accounting literature has been estimating structural observations for decades --- every ERC regression, every FERC analysis, every measure of ``price informativeness'' is an estimate of how components relate to price. This framework suggests a new interpretation of these estimates: they are not merely measures of ``how efficiently'' the market prices earnings, but partial solutions to the identification problem. An investor who knows the ERC for earnings can, when the next price movement occurs, make a better inference about how much of it is attributable to earnings versus other factors. The ERC does not tell you what earnings are; it tells you how earnings /relate/ to the aggregate, which narrows the range of plausible decompositions.

  This typology also has implications for what it means to possess ``private information.'' The standard assumption in the literature is that private information consists of component observations --- an insider knows $\theta_i$ before the market does. But there are two additional forms of private information that may be equally valuable for resolving the identification problem.

  A manager who knows her own firm's earnings, costs, and investment plans has extensive component observations. But she also possesses structural knowledge: she knows /how/ these components combine to produce value, which components are persistent and which are transitory, and how changes in one component affect others. This structural knowledge allows her to decompose the price signal in ways that an outside observer, who lacks the structural map, cannot.

  This provides a more precise answer to the question raised by citet:gelsominLearningHypothesisRevisited2023 regarding whether managers learn from price. Managers /can/ learn from price, but only about dimensions they are able to isolate using their supplementary signals --- both component observations and structural knowledge. A manager who knows her own earnings can observe price and infer the market's assessment of everything else: $\theta_{\text{rest}} = V - \theta_{\text{earnings}}$. A manager who additionally knows the persistence structure of her firm can make inferences about future price dynamics that outside investors cannot. But a manager who does not know, say, the market's discount rate or the competitive dynamics of a new market segment still faces an underdetermined system on those dimensions.

  As a result, Gelsomin and Hutton's Assumption 3 --- that managers extract private information from prices --- is not categorically wrong, but it may be /conditionally/ true: managers can extract information only along dimensions that their own private signals allow them to decompose.

  The same logic applies to other market participants. Analysts who specialize in an industry possess structural knowledge about how that industry's components combine and persist, enabling them to decompose prices more effectively than generalists. Institutional investors with proprietary models possess structural estimates (ERCs, factor loadings, persistence parameters) that constrain their solution sets. Retail investors, who typically observe only price and headline news, face the most severe identification problem. This creates a natural hierarchy of decomposition ability --- from managers (most supplementary signals) to specialized analysts to institutional investors to retail investors --- that maps to empirically observed patterns in forecast accuracy and trading performance.

  These observations suggest a way of understanding the value of disclosure. The contribution of each disclosure is not merely ``more information'' in a generic sense; it is the resolution of a specific dimension of the identification problem that price alone cannot resolve. Disaggregated disclosures are more valuable than aggregated ones because each separately reported component helps narrow the solution set. This is consistent with why citet:ramananPromotingInformativenessStaggered2015 finds that staggered information releases are more informative than simultaneous releases: each release, individually priced, resolves one dimension before the next arrives, whereas simultaneous release produces a single aggregate price response that conflates all dimensions.

  The implication for empirical research design is that studies assuming investors learn individual parameters from aggregate price movements are implicitly assuming that the identification problem has been resolved. The results suggest that this assumption should be stated explicitly and defended, not taken as a default. The degree to which investors can learn individual parameters depends on the availability of supplementary signals --- direct disclosures, analyst decompositions, media coverage, and social media discussion --- that help resolve the underdetermined system. These results do not imply that investors /never/ learn individual parameters; they imply that investors cannot do so from price alone, and that the informational ecosystem surrounding a security determines how much of the identification problem can be resolved in practice.

* Conclusion
  This paper extends prior models, in which investors perfectly learn a single valuation parameter from price, to a setting in which price responds to multiple valuation parameters. The analysis shows that this creates an identification problem: a single price constrains investors to an \(n-1\)-dimensional hyperplane of parameter vectors, all of which are consistent with the observed price (Proposition \ref{prop:nonid}). When valuation components evolve at different rates, investors who cannot decompose the aggregate are forced to apply average persistence, generating forecast errors that increase in both the number of components and the dispersion of their persistence rates (Propositions \ref{prop:forecast}--\ref{prop:mse}). Supplementary signals reduce these errors, with each signal's value depending on which component it reveals (Proposition \ref{prop:collapse}, Corollary \ref{cor:marginal}). The agent-based simulation confirms that the market converges on the correct aggregate price while maintaining significant dispersion in individual parameter estimates, and that component-level errors increase with $n$. Aggregate market efficiency --- in which price reflects all available information --- is therefore distinct from the market efficiently pricing the individual informational components that compose that value.

  This identification constraint is qualitatively different from the impediments to learning in prior noisy rational expectations models (citet:grossmanImpossibilityInformationallyEfficient1980, citet:fischerReportingBias2000, citet:felthamPerformanceMeasureCongruity1994). In those settings, incomplete learning arises from exogenous noise perturbing a fundamentally identified signal; removing the noise restores perfect learning. Here, even without noise, a single price cannot reveal individual parameters when $n > 1$. The dimensional principle underlying this result is well established in general equilibrium theory (citet:radnerRationalExpectationsEquilibrium1979, citet:jordanGenericExistenceRational1982, citet:allenStrictRationalExpectations1982), but has not previously been applied to single-security investor learning or connected to the accounting literature's assumptions about how investors process earnings components. This does not imply that markets are inefficient or that investors can never learn individual components. It implies that component learning cannot be inferred from aggregate price convergence alone.

  As developed in the Discussion, this distinction reframes common anomaly evidence. Processing costs, limited attention, and behavioral fixation remain relevant, but they operate within a structural bound on what a single price signal can reveal. The formal results also generate direct empirical predictions: anomaly magnitudes should scale with $n\sigma^2\text{Var}_n(\boldsymbol{\alpha})$ --- a multiplicative interaction that discriminates among competing explanations, since processing-cost models predict anomalies increasing in $n$ alone while behavioral fixation models predict anomalies increasing in persistence heterogeneity alone --- and market responses to component disclosures should be larger when the disclosed component's persistence is further from the mean of the remaining unknowns. More broadly, the model predicts a hierarchy of forecast accuracy across investor classes that reflects differential access to supplementary decomposition signals.

  The model is deliberately simple to isolate the identification mechanism. Other learning models may extract additional information from time-series data or trading volume, but no mechanism can allow /perfect/ recovery of multiple contemporaneous valuation parameters from a single contemporaneous price. To the extent that markets are an information technology as well as a means of allocating capital, understanding how price interacts with disclosures, analyst work, and peer information channels remains central.

  One promising direction concerns social media platforms where investors share fundamental analysis. Sites such as Seeking Alpha host detailed component-level discussions --- segment breakdowns, earnings quality assessments, valuation models --- that could in principle help investors resolve the identification problem. Existing research shows that these platforms provide informative signals: aggregate opinion predicts returns and earnings surprises (citet:chenWisdomCrowdsValue2014), disclosed financial positions enhance the credibility of published analysis (citet:campbellSkinGamePersonal2019), and crowdsourced earnings forecasts contain information beyond professional analyst consensus (citet:jameValueCrowdsourcedEarnings2016). However, this literature has measured whether social media improves /aggregate/ predictions, not whether it enables investors to distinguish between /individual components/ of value. Whether peer-to-peer fundamental analysis serves as a decomposition mechanism --- helping investors sort out which components of value are driving price rather than merely improving aggregate forecasts --- is an open empirical question.

#+latex: \newpage
\singlespacing
* Appendices
** Simulation Implementation
  :PROPERTIES:
  :CUSTOM_ID: a_sim
  :END:

  This appendix provides the complete implementation of the agent-based simulation described in Section 3. The code serves two purposes: it provides a concrete, executable specification of the model, and it enables replication and extension of the results.

  Following the assumptions stated above, the implementation creates two types of investor. Operationally, a fully informed investor's prior is equal to the true value of the security. An uninformed investor's prior consists of independent draws from $\mathcal{N}(0,1)$. The following function creates either an informed or uninformed investor, depending on its arguments:

\singlespacing
#+BEGIN_SRC clojure :results value silent
  (defn make-investor
    ([n] ;; Makes an uninformed agent (Assumption 3)
     {:prior (vec (repeatedly n (fn [] {:informed? false :value (draw-round)})))
      :id (uuid)})
    ([v fully?] ;; Makes an informed agent from a vector of parameters
     {:prior
      (if fully?
        (mapv (fn [pr] {:informed? true :value pr}) v)
        (let [i (rand-int (count v))]
          (assoc (:prior (make-investor (count v)))
                 i
                 {:informed? true :value (get v i)})
          ))
      :id (uuid)})
    )
#+END_SRC
\doublespacing

See [[#a_util][Appendix B]] for definitions of helper functions such as draw-round.

When called with an integer, /n/, the above function creates an uninformed investor with a randomly-generated prior containing /n/ parameters (Assumption \ref{a:information}). When called with a vector prior /v/ and with /fully?/ set to true, the function creates a fully informed investor whose prior is set to the vector prior. The function also contains an option to create a partially informed investor: if /fully?/ is set to false, the function randomly chooses to make the investor informed about one parameter in the vector prior. This option is included for potential extensions; all results reported in this paper use the fully informed/fully uninformed specification described in Assumption \ref{a:investors}.

As stated in Assumption \ref{a:investors}, the number of informed investors is set to a fixed fraction $\lambda$ of the total. Consistent with the view that price constitutes an average consensus belief of traders (citet:verrecchiaConsensusBeliefsInformation1980), $\lambda > 0.5$ results in consistent convergence to true price. The implementation sets $\lambda = 0.6$ and $N = 500$, yielding 300 informed and 200 uninformed investors. The following variables and functions create a vector of investors according to this design:

\singlespacing
#+BEGIN_SRC clojure :results value silent
  (def investor-num 500) ;; N = 500 (Assumption 2)
  (def inform-frac 0.6) ;; lambda = 0.6 (Assumption 2)
  (defn make-investor-list
    [n inf-prior]
    (into [] (concat (repeatedly (* investor-num inform-frac)
                                 (fn [] (make-investor inf-prior true)))
                     (repeatedly (* (- 1 inform-frac) investor-num)
                                 (fn [] (make-investor n))))))
#+END_SRC
\doublespacing

These investors trade in a simulated market. The following function creates the market, implementing Assumptions \ref{a:security} and \ref{a:price}:

\singlespacing
#+BEGIN_SRC clojure :results value silent
(defn make-market
  [n]
  (let [security (vec (repeatedly n draw-round)) ;; theta_i ~ N(0,1), rounded
        start-price (+ (draw-round) (reduce + security))] ;; p_0 = V + eta
    {:security security
     :price start-price
     :sp [start-price]
     :investors (make-investor-list n security)}
    ))
#+END_SRC
\doublespacing

The argument /n/ specifies the number of parameters $n$ that determine the value of the security, and thus the number of parameters in each investor's prior. Parameter values are drawn from $\mathcal{N}(0, 1)$ and rounded to two decimal places (establishing a minimum tick size of 0.01). The starting price is set by summing the valuation parameters and adding a draw from $\mathcal{N}(0, 1)$, so that $p_0 = V + \eta$. The function then creates the investors as specified above.

The following functions update the market price based on investor demand for shares. As stated in Assumptions \ref{a:trading} and \ref{a:price}, investors submit unit orders $d_j^t \in \{-1, 0, 1\}$ based on whether their aggregate prior exceeds the current price, and the market adjusts price by $\pm \delta$ based on the sign of the net order imbalance.

\singlespacing
#+BEGIN_SRC clojure :results value silent

  (defn- order-update
    [m]
    (let [;; Implements Assumption 5: d_j^t based on prior vs. price
          pe (reduce + (mapv (fn [a]
                               (let [prior (reduce + (prior-vals a))]
                                 (cond
                                   (> (:price m) prior) -1
                                   (< (:price m) prior) 1
                                   :else 0)))
                             (:investors m)))]
      ;; Implements Assumption 4: p_{t+1} = p_t + delta * sgn(sum of orders)
      (+ (:price m) (cond
                      (> pe 0) 0.01
                      (< pe 0) -0.01
                      :else 0)))
    )

  (defn rand-prior-update
    [m pred upd]
    (mapv (fn [a]
            ;; the predicate determines
            ;; if the agent updates (Assumption 6).
            (let [pred (pred m a)]
              (cond
                ;; if agent is informed or predicate is not satisfied,
                ;; do not update.
                (or (not pred) (informed-agent? a)) a
                ;; otherwise, randomly select an uninformed parameter
                ;; and apply the update function (Assumption 6, steps 1-3).
                :else (let [pri (:prior a)
                            ;; Step 1: select parameter k uniformly at random
                            i ((fn [v]
                                 (let [r (rand-int (count v))]
                                   (cond
                                     (every? :informed? v) nil
                                     (:informed? (get v r)) (recur v)
                                     :else r)
                                   )) pri)
                            sel-pri (get pri i)
                            ;; Step 2: compute inferred value
                            ;; tilde-theta_k = p_t - sum_{i != k} hat-theta_i
                            adjust-price-for-prior
                            (- (:price m)
                               (- (reduce + (map :value pri))
                                  (:value sel-pri)))]
                        ;; Step 3: average current estimate with inferred value
                        (assoc a :prior
                               (update
                                pri
                                i
                                #(assoc
                                  %
                                  :value
                                  (upd adjust-price-for-prior sel-pri))))))))
          (:investors m)))

  ;; does price move towards the agent's prior?
  (defn does-not-move-toward?
    [m a]
    (let [abs-val (fn [p] (Math/abs (- (reduce + (prior-vals a)) p)))]
      (not (< (abs-val (:price m)) (abs-val (last (:sp m)))))
      )
    )

  (defn market-update
    [m]
    (-> m
        ;; adds price to history
        (assoc :sp (conj (:sp m) (:price m)))
        ;; runs the order update function above
        (assoc :price (order-update m))
        ;; updates investors (Assumption 6)
        (assoc :investors (rand-prior-update
                           m
                           ;; predicate: update if price does not move toward prior
                           does-not-move-toward?
                           ;; update function: average current with inferred value
                           (fn [p prior] (/ (+ (:value prior) p) 2))))
        )
    )
#+END_SRC
\doublespacing

As stated in Assumption \ref{a:updating}, informed investors do not update. Uninformed investors observe the period-over-period price change and decide to revise a parameter of their prior if the price fails to move towards what they deem to be the correct price. Otherwise, they assume that their prior is accurate.

When their prior contains only a single parameter ($n = 1$), uninformed investors update that parameter. Otherwise, uninformed investors are unable to distinguish from price movements which parameter of their prior is incorrect --- this is a direct consequence of the identification problem stated in Proposition \ref{prop:nonid} --- and revise a random parameter from their prior vector. They compute the inferred value of the selected parameter by subtracting their estimates of all other parameters from the observed price, then average their current estimate with this inferred value. As a result, uninformed investors slowly adjust their prior to match the price signal they receive, but their parameter-level adjustments are based on an incomplete decomposition of the price signal.

The following code initializes the market and allows investors to trade in rounds until the market converges on a price:

\singlespacing
#+BEGIN_SRC clojure :results value silent

  ;; the market is presumed to have converged
  ;; when the price exhibits no more than
  ;; two values over 10 trading rounds.
  (defn converge?
    [m]
    (some #(< % 3) (map (comp count frequencies) (partition 10 1 (:sp m))))
    )

  ;; run market-update if the market has not converged.
  ;; s indicates the number of valuation parameters.
  ;; n indicates the number of markets to initialize.
  (defn iterate-markets
    [s n]
    (map #(take-while (comp not converge?) (iterate market-update %))
         (take n (repeatedly #(make-market s)))))

#+END_SRC
\doublespacing

The /converge?/ function calculates a rolling window of length 10 on the market's price history and counts the number of unique prices observed. If that number is less than 3, the function concludes that the market has converged; because the tick size is set to $\delta = 0.01$, some markets converge on a pattern where they move up and down between two prices that are different by only that amount, but price is functionally unchanged period over period.

*** Results Generation

  The following code generates the simulation results presented in Section 4.

\singlespacing
#+BEGIN_SRC clojure :results value silent

  (def m1 (first (iterate-markets 1 1)))
  (price-plot m1)

#+END_SRC

#+BEGIN_SRC clojure :results value silent

  (def m2 (first (iterate-markets 2 1)))
  (price-plot m2)

#+END_SRC

#+BEGIN_SRC clojure :results value silent
  ;; The vector argument indicates the progress to convergence.
  ;; So 0 indicates the first round, 1 the final round.
  ;; 0.2 indicates 20% of the way to the final round.
  ;; The final numeric parameter designates the number of columns
  ;; for the combined plot.
  (plot-snap m2 [0 0.2 0.4 0.6 0.8 1] 3)

#+END_SRC
\doublespacing

  The following function calculates the average of investors' estimates for each parameter in the valuation vector:

\singlespacing
#+BEGIN_SRC clojure :results output silent

  (defn avg-prior
    [m]
    (map (fn [n] (/ (reduce + (map #(get-in % [:prior n :value]) (:investors m)))
                    (count (:investors m))))
         (range (count (:security m)))))

#+END_SRC
\doublespacing

  The following code computes the mean squared error of parameter estimates for each market size and performs the statistical tests reported in Table [[tbl:sse-errors]]:

\singlespacing
#+NAME: mean-error
#+BEGIN_SRC clojure :results value table :eval never-export

  (defn sse-avg
    [m]
    (/ (reduce + (map #(* (- %1 %2) (- %1 %2))
                      (:security m) (avg-prior m)))
       (count (:security m))))
  (def sse-avg-by-size (mapv (fn [s]
                              (map sse-avg
                                   (map last (iterate-markets s 500))))
                            (rest (map inc (range 10)))))

  (vec (conj (map #(vector %1 %2 %3 %4)
                  (vec (rest (mapv inc (range 10))))
                  (mapv (comp #(round2 3 %) average) sse-avg-by-size)
                  (vec
                   (conj
                    (map #(round2 3 (:t-stat
                                   (stats/t-test
                                    (get sse-avg-by-size (- % 2))
                                    :y (get sse-avg-by-size 0))))
                         [3 4 5 6 7 8 9 10]) ""))
                  (vec (conj
                        (map #(round2 3 (:t-stat
                                         (stats/t-test
                                          (get sse-avg-by-size (- % 2))
                                          :y (get sse-avg-by-size (- % 3)))))
                             [3 4 5 6 7 8 9 10]) "")))
             ["Number of Parameters"
              "Mean MSE"
              "t-stat vs. n = 2"
              "t-stat vs. n - 1"]))

#+END_SRC
\doublespacing

** Utilities
  :PROPERTIES:
  :CUSTOM_ID: a_util
  :END:
  These variables and functions support the code in the paper. If you are reproducing the code in the paper you should define these variables and functions *first*.

  Like other Lisps, Clojure has a simple standard syntax. Function calls are enclosed in parentheses. The first object within a set of parentheses is called as a function, the remaining objects are arguments. Functions are first-class objects in Clojure, allowing them to be treated as data and passed as arguments. Clojure also supports the use of anonymous functions using the (fn...) or #(...) syntax.

#+BEGIN_SRC clojure :results value silent
  (require '[oz.core :as oz] ;; version "1.6.0-alpha5"
           '[incanter.stats :as stats] ;; version "1.9.3"
           )

  (def norm-mean 0)
  (def norm-std 1)
  ;; Draws from a normal distribution with a mean of norm-mean (defined above)
  ;; and a standard deviation of norm-std (defined above).
  (defn draw-incant-norm
    []
    (stats/sample-normal 1 :mean norm-mean :sd norm-std))

  (defn round2
    "Round a double to the given precision (number of significant digits)"
    [precision d]
    (let [factor (Math/pow 10 precision)]
      (/ (Math/round (* d factor)) factor)))

  ;; Rounds a number to two digits,
  ;; for a minimum tick size of 0.01.
  (defn draw-round
    []
    (round2 2 (draw-incant-norm)))

  (defn average
    [xs]
    (float (/ (apply + xs) (count xs))))
  (defn uuid
    []
    (keyword (str (java.util.UUID/randomUUID))))

  (defn prior-vals
    [a]
    (mapv :value (:prior a)))

  (defn informed-agent?
    [a]
    (some :informed? (:prior a)))

  (defn make-data
    [ms]
    (vec (mapcat #(let [priors (mapv (fn [a] (reduce + (map :value (:prior a))))
                                     (:investors %2))
                        infs (mapv (fn [a] (:informed? (first (:prior a))))
                                   (:investors %2))]
                    (mapv (fn [prior inf] (hash-map :time %1
                                                    :price (:price %2)
                                                    :security
                                                    (reduce + (:security %2))
                                                    :prior prior
                                                    :informed? inf))
                          priors
                          infs))
                 (range (count ms)) ms))
    )

  (defn price-plot
    [m]
    (oz/view! {:height 600
               :width 800
               :data {:values (make-data m)}
               :layer [{:encoding {:x {:field "time"}
                                   :y {:field "prior"}
                                   :color {:field "informed?"}}
                        :mark {:type "point"}}
                       {:encoding {:x {:field "time"}
                                   :y {:field "price"}}
                        :mark {:type "line"}}]
               }))

  (defn make-snap-data
    [ms]
    (vec (mapcat #(let [priors (mapv (fn [a] (prior-vals a))
                                     (:investors %2))
                        infs (mapv (fn [a] (mapv :informed? (:prior a)))
                                   (:investors %2))]
                    (mapv (fn [prior inf] (hash-map :time %1
                                                    :price1 (first (:security %2))
                                                    :price2 (second (:security %2))
                                                    :prior1 (first prior)
                                                    :prior2 (second prior)
                                                    :informed? (if (first inf)
                                                                 true
                                                                 false)
                                                    ))
                          priors
                          infs))
                 (range (count ms)) ms)))

  (defn plot-snap
    [d v col-num]
    (let [sd (make-snap-data d)
          make-plot (fn [n] {:data {:values (filter #(= (:time %) n) sd)}
                             :title (str "round = " n)
                             :layer
                             [{:encoding {:x {:field "prior1"}
                                          :y {:field "prior2"}
                                          :color {:field "informed?"}}
                               :mark {:type "point" :opacity 0.3}}
                              {:encoding {:x {:field "price1"}
                                          :y {:field "price2"}}
                               :mark {:type "point" :shape "square" :size 60}}]
                             })
          length (- (count (distinct (map :time sd))) 1)
          pv (partition col-num (mapv #(int (* % length)) v))]
      (oz/view! {:vconcat (map (fn [row] {:hconcat (map make-plot row)}) pv)})))
#+END_SRC

#+latex: \newpage
\printbibliography


#+latex: \newpage
* Figures and Tables

#+CAPTION: Price convergence for a market with one parameter, as in GS
#+NAME: fig:1-param-price
[[./plots/1-param-price.png]]


#+CAPTION: Price convergence for a market with two parameters
#+NAME: fig:2-param-price
[[./plots/2-param-price.png]]

#+CAPTION: Prior updating by investors with two parameters
#+NAME: fig:2-param-investors
[[./plots/2-param-investors.png]]

#+CAPTION: Mean Squared Error of Parameter Estimates as the Number of Parameters Increases
#+NAME: tbl:sse-errors
#+ATTR_LATEX: :environment longtable :align |ccC{3cm}C{3cm}|
|-------------+----------------------------+----------------------------------------------------|
| Number of Parameters | Mean MSE | t-stat vs. $n = 2$ | t-stat vs. $n - 1$ |
|           2 |    0.078 |                                                |                                                  |
|           3 |    0.109 |                                          4.426 |                                            4.426 |
|           4 |    0.129 |                                           7.34 |                                             2.85 |
|           5 |    0.131 |                                          8.406 |                                            0.312 |
|           6 |    0.141 |                                         10.145 |                                            1.812 |
|           7 |    0.139 |                                         10.061 |                                           -0.246 |
|           8 |    0.142 |                                         10.624 |                                            0.418 |
|           9 |    0.137 |                                         10.332 |                                           -0.918 |
|          10 |     0.15 |                                         12.112 |                                             2.74 |
|-------------+----------------------------+----------------------------------------------------|

* Extra :noexport:
# Result for (market-convergence-numbers (mapv inc (range 20))) when the number of markets is 500:
# (86.924
#  89.912
#  88.298
#  87.372
#  90.438
#  89.946
#  90.848
#  93.546
#  88.362
#  89.922
#  94.882
#  87.132
#  88.974
#  93.0
#  90.21
#  89.038
#  86.07
#  90.894
#  84.3
#  87.45)
# Differences are not statistically significant, so it is generally not true that the market takes longer to converge when there are more pieces of information in play.

# Result for average error for uninformed investors as market size grows.
# Code: (map (fn [i] (/ (reduce + (map #(prior-error (last %)) (iterate-markets i 100))) 100)) (map inc (range 20)))
# (0.005397382785367705
#  0.39579183265378426
#  0.41453329430878716
#  0.44119554923490667
#  0.46768149660663433
#  0.4521248587112712
#  0.5086778009442662
#  0.45969876676716326
#  0.4800114462748398
#  0.4840139420813182
#  0.4996159071353425
#  0.49923822423525543
#  0.4879302380814084
#  0.502249419219274
#  0.4844871103781026
#  0.4940053428192927
#  0.525746339625341
#  0.479169751808412
#  0.4730871843721022
#  0.4879026433065603)
