#+TITLE: Analysis of Referee Report JOAR-2019-358
#+SUBTITLE: "Sympathy for the Noise Trader: Limitations of Learning from Price"
#+AUTHOR: Review Analysis
#+DATE: 2026-02-06
#+OPTIONS: toc:t num:t

* Overview

This document provides a detailed analysis of the referee report for JOAR-2019-358, submitted to the /Journal of Accounting Research/ for the 55th Annual (2020) JAR Conference. The referee raised three substantive criticisms. We evaluate each against the relevant literature.

The referee's core claims are:
1. The model is incomprehensible and too distant from Grossman & Stiglitz (1980) to speak to debates about rational equilibrium.
2. Convergence without full rationality is already known (Gode & Sunder 1993), and incomplete learning is unsurprising given non-optimal agents.
3. The multi-signal nature of learning from price is well-known and can be shown theoretically.

Our conclusion: Criticism 1 identifies real presentation problems that should be fixed. Criticisms 2 and 3, however, mischaracterize the paper's contribution and are not supported by the papers the referee cites.

* Criticism 1: "The Model is Incomprehensible"

** What the Referee Says

#+BEGIN_QUOTE
The assumptions are not clearly stated, and the reader is expected to understand what is done by deciphering (what I think) is a java-based code. Even if the paper is a simulation, readers expect to know the distribution of the data-generating process and the equilibrium using conventional language (i.e., text and mathematics) and code is not a replacement for assumptions.
#+END_QUOTE

The referee also lists four specific departures from GS:
1. No information acquisition
2. Beliefs set exogenously without Bayes plausibility
3. Traders buy or sell a single share
4. No noise traders

** Assessment: Partially Fair, but Overstated

*** The presentation critique is legitimate

The paper does need a formal mathematical statement of its assumptions before presenting code. This is standard practice in accounting and finance. The code should supplement the mathematics, not replace it. The referee's identification of the language as "java-based" (it is Clojure, a Lisp dialect) underscores that the code alone is not communicating effectively. This is entirely fixable.

*** The claim that the model is "not anywhere close to GS" is overstated

The core setup --- informed investors who know a parameter, uninformed investors who must learn it from price, and a market that iterates toward convergence --- is directly from GS. The paper explicitly states: "It is not my intention to perfectly replicate GS' model, but rather to create a similar set of circumstances for investor learning." The departures, examined individually:

**** No information acquisition

True, but the paper studies what happens /after/ the informed/uninformed split, not the decision to become informed. GS's information acquisition result (the "impossibility" of perfectly efficient markets because information gathering becomes unprofitable) is a separate contribution from the learning mechanism that communicates information through price. The paper is concerned with the latter. Setting the informed fraction exogenously is consistent with how GS's own model operates once equilibrium is reached: there is a fixed fraction of informed traders, and the question is how well uninformed traders learn from them.

**** Exogenous beliefs without Bayes plausibility

This is a fair technical point. The uninformed investors' priors are drawn from $N(0,1)$ independently, which need not satisfy Bayes plausibility (i.e., the prior distribution of beliefs need not integrate to the true distribution of fundamentals). However:
- In GS, uninformed investors also have priors that are not derived from fundamentals --- they are simply "uninformed" and must learn from price.
- The paper could address this by constraining uninformed priors to be draws from the same distribution that generates fundamentals, ensuring Bayes plausibility. This is a robustness check, not a fundamental flaw.

**** Single-share trading

A simplification relative to GS, where demand is continuous and depends on risk aversion. However, the paper's core finding --- the identification problem of n parameters from 1 price --- does not depend on demand elasticity. Whether an investor buys 1 share or 100, the price signal still contains only aggregate information. Continuous demand would change the /rate/ of convergence but not the structural impossibility of decomposing the aggregate into components.

**** No noise traders

This is actually more interesting than damning. In GS, noise traders are /required/ to prevent perfect revelation of $\theta$ through price. Without noise, price perfectly reveals $\theta$, and no one needs to become informed, collapsing the market.

The paper's implicit (and unstated) insight is that with multiple parameters, you do not /need/ exogenous noise to prevent perfect revelation. The identification problem itself prevents it: even if price perfectly converges to the true aggregate value, investors still cannot decompose it into individual parameters. The "noise" is structural, arising from the dimensionality mismatch between the parameter space and the signal space. This should be stated as a contribution rather than left for the referee to discover.

** Recommended Response

Revise the paper to:
1. State all model assumptions in formal mathematical notation before presenting code.
2. Explicitly acknowledge departures from GS and explain why each is either immaterial to the core result or interesting in its own right.
3. Frame the absence of noise traders as a feature: the identification problem replaces exogenous noise as the barrier to perfect learning.

* Criticism 2: "Convergence Without Rationality is Known"

** What the Referee Says

#+BEGIN_QUOTE
The observation that agent without assuming full rationality can converge to equilibrium is well-known. This result can be found in, for example, Gode and Sunder (1993), uncited, in contexts where convergence to the equilibrium seems less obvious. [...] The second part of the contribution is that agents do not fully learn from price; but this is not very surprising if agents are not fully optimal and create noise from restricted trading rules with incorrect priors.
#+END_QUOTE

** Assessment: Misdirected

*** Gode & Sunder (1993) does not address the paper's contribution

Gode & Sunder (1993), "Allocative Efficiency of Markets with Zero-Intelligence Traders," published in the /Journal of Political Economy/, shows that "zero-intelligence" (ZI) traders submitting random bids and offers in a continuous double auction achieve near-perfect allocative efficiency, provided they are budget-constrained.

This paper is entirely orthogonal to the question at hand:

| Feature | Gode & Sunder (1993) | This Paper |
|---------|---------------------|------------|
| Information asymmetry | None. All traders have fixed, known private values. | Yes. Informed vs. uninformed investors. |
| Learning from price | None. Traders submit random orders. They do not observe or use price information to update beliefs. | Central to the model. Uninformed investors revise priors based on price movements. |
| Bayesian updating | None. | Approximated via averaging update rule. |
| Multiple parameters | No. Single good, single value per trader. | Yes. Security value is a vector of n parameters. |
| Core question | Can market mechanisms alone produce efficient prices without intelligent traders? | Can investors learn /individual/ valuation parameters from an /aggregate/ price signal? |

Gode & Sunder is a paper about /market mechanism design/. It asks whether the double auction institution can substitute for individual trader intelligence in achieving allocative efficiency. It does not involve any form of information aggregation, learning, or parameter estimation. Citing it as preempting a paper about multi-parameter investor learning from price is a category error.

*** Incomplete learning is not an artifact of agent suboptimality

The referee suggests that incomplete learning is "not very surprising if agents are not fully optimal and create noise from restricted trading rules with incorrect priors." This framing implies that smarter agents would learn perfectly. This is incorrect.

The structural issue is one of identification: a single price embodies the sum of n parameters, and any parameter vector that produces that sum is observationally equivalent. This is not a problem of agent intelligence --- it is a mathematical constraint:

$$\text{Given: } p = \sum_{i=1}^{n} \theta_i$$

An observer of $p$ alone cannot recover the individual $\theta_i$ for $n > 1$, regardless of the sophistication of the estimation method. A Bayesian optimizer, a maximum-likelihood estimator, or a neural network would all face the same constraint: the system is underdetermined.

Making agents more sophisticated (e.g., Bayesian with correct priors, or using regression on price history) could improve how efficiently they use the price signal and might produce different /distributions/ of individual parameter errors. But it cannot eliminate the fundamental identification problem. The paper could be strengthened by making this point explicitly and demonstrating robustness to alternative updating rules.

** Recommended Response

1. Cite Gode & Sunder (1993) and explain why it addresses a different question.
2. State the identification problem formally: one equation, n unknowns, infinitely many solutions.
3. Consider showing results under alternative updating rules (e.g., Bayesian, OLS) to demonstrate that the result is structural, not an artifact of the simple averaging rule.

* Criticism 3: "Multi-Signal Learning Impossibility is Known"

** What the Referee Says

#+BEGIN_QUOTE
The multi-signal nature of learning from price is also well-known in the muddling literature in rational expectations and can be shown theoretically without need for a full-on simulation. If there is a single signal to learn from but an n-dimensional original information set, the signal will not fully reveal the entire information set. This is certainly true in GS where the fundamental information is perturbed by noise traders; or Fischer and Verrecchia (2000) with an incentive shock perturbing the learning; or in multi-signal LENs where the effort is perturbed by noise (Feltham and Xie 1994). These mechanisms breaking how we can invert private information from a single signal are now well-known.
#+END_QUOTE

** Assessment: Partially Fair in Principle, but the Cited Papers Do Not Preempt the Contribution

The referee's core intuition --- that a single signal cannot fully reveal an $n$-dimensional information set --- is correct as a general principle, and the theoretical literature (Radner 1979, Jordan 1982) does establish the dimensional constraint. However, the /specific papers the referee cites/ do not address the same structural problem, and the paper's contribution is not preempted by them. We examine each in detail.

*** Fischer & Verrecchia (2000): "Reporting Bias"

**** What the paper does

Fischer & Verrecchia model a setting where a manager privately observes the firm's true earnings (a /single scalar/) and issues a biased report. The market, knowing the manager has an incentive to bias upward, rationally discounts the report. The paper derives the earnings response coefficient (the slope of price on reported earnings) and shows how it varies with the cost of biasing and the precision of the manager's private information.

**** Information structure

- One unknown: firm value (scalar)
- One signal: biased earnings report (scalar)
- One price
- The market's problem: filter a single signal through a known bias structure

**** Does it address multi-parameter learning from price?

No. The perturbation to learning in Fischer & Verrecchia comes from /strategic bias/, not from a dimensionality mismatch. There is a single parameter (true earnings), a single report, and a single price. The market can (and does) rationally discount the bias. The identification problem is 1-to-1, merely perturbed by noise. This is fundamentally different from the n-to-1 identification problem in the paper under review.

*** Feltham & Xie (1994): "Performance Measure Congruity"

**** What the paper does

Feltham & Xie study multi-task principal/agent contracts using the LEN (Linear-Exponential-Normal) framework. An agent exerts effort across multiple tasks. The principal observes performance measures that imperfectly capture the agent's multi-dimensional effort. The paper characterizes when multiple performance measures are valuable and how incongruent measures distort effort allocation.

**** Information structure

- Multiple unknowns: the agent's effort vector $\mathbf{e} = (e_1, \ldots, e_n)$
- Performance measures: linear functions of effort plus noise
- The principal's problem: design a contract that incentivizes the right effort allocation

**** Does it address multi-parameter learning from price?

No. There is a structural /analogy/: a single performance measure cannot capture multi-dimensional effort, just as a single price cannot reveal multi-dimensional valuation parameters. However:

1. The /domain/ is entirely different. Feltham & Xie is about /contracting/ (how to incentivize a known agent), not about /market learning/ (how investors extract unknown information from prices).
2. The /mechanism/ is different. In Feltham & Xie, the principal knows the mapping from effort to performance measures. The problem is designing incentives, not inferring unknowns.
3. The /implications/ are different. Feltham & Xie concludes that multiple performance measures are needed for congruent incentives. The analogous conclusion for markets --- that multiple signals are needed for investors to learn individual parameters --- is precisely the paper's contribution to the /market learning/ literature, which has not drawn this conclusion.

Citing Feltham & Xie as preempting a result about market learning is like citing the Heisenberg uncertainty principle as preempting an empirical study showing that a particular measurement apparatus cannot simultaneously measure position and momentum. The general principle may be known in one domain, but its application and implications in another domain constitute a contribution.

*** Grossman & Stiglitz (1980)

The referee says the impossibility of full revelation "is certainly true in GS where the fundamental information is perturbed by noise traders." But in GS, the perturbation comes from /exogenous noise traders/, not from a dimensionality mismatch. The information structure is:

$$u = \theta + \epsilon$$

Price is a noisy signal of the /single/ parameter $\theta$. The noise is added by noise traders, not created by the structure of the information space. GS does not consider the case where $\theta$ is a vector and price is a scalar. In fact, the entire mechanism of GS assumes that price /would/ perfectly reveal $\theta$ in the absence of noise --- that is the "impossibility" result. The paper under review shows that even without noise, price cannot reveal individual components when $\theta$ is a vector.

*** What the broader literature actually shows

A survey of the rational expectations and information aggregation literature reveals that the standard models maintain a 1:1 ratio between unknown parameters and price signals:

| Paper | Unknown Parameters | Price Signals | Ratio |
|-------|-------------------|---------------|-------|
| Grossman (1976) | 1 ($\theta$) | 1 (price) | 1:1 |
| Grossman & Stiglitz (1980) | 1 ($\theta$) | 1 (price + noise) | 1:1 |
| Verrecchia (1980) | 1 (asset return) | 1 (price) | 1:1 |
| Diamond & Verrecchia (1981) | 1 (asset return) | 1 (price) | 1:1 |
| Admati (1985) | K (multi-asset returns) | K (multi-asset prices) | 1:1 |
| Blume & Easley (1982) | K | K | 1:1 (explicitly required) |

Critically, Blume & Easley (1982) /explicitly state/ that signals and prices must be paired for Bayesian learning to function (pages 343--344). Diamond & Verrecchia (1981) model many traders each with a noisy signal about a /single scalar/ --- the aggregation is many-signals-to-one-parameter, which is well-identified.

Admati (1985) extends the framework to multiple assets, but with K assets there are K prices, preserving the 1:1 ratio. The case where a /single/ price must reveal /multiple/ parameters is simply not addressed.

*** The closest paper: Avery & Zemsky (1998)

The paper most relevant to the identification problem is Avery & Zemsky (1998), "Multidimensional Uncertainty and Herd Behavior in Financial Markets," published in the /American Economic Review/. They show that:

- With one dimension of uncertainty, a single price prevents herding (the price adjusts to reflect information, removing the incentive to herd).
- With two or three dimensions of uncertainty (value uncertainty, event uncertainty, and composition uncertainty), herding and mispricing arise /precisely because the single price mechanism cannot disentangle the multiple dimensions/.

This directly illustrates the identification failure. However:
1. Avery & Zemsky frame it as a /herding/ result, not as a general result about the limits of learning individual parameters from price.
2. They do not measure the /extent/ of parameter-level mispricing or how it scales with dimensionality.
3. They are not cited by the referee.

The existence of Avery & Zemsky (1998) shows that the /intuition/ is present in the literature, but the specific application to investor learning about individual valuation parameters --- and the empirical accounting literature's assumption that such learning occurs --- has not been explored.

*** The referee's logical error

The referee's argument contains a subtle logical error. The referee says:

#+BEGIN_QUOTE
These mechanisms breaking how we can invert private information from a single signal are now well-known.
#+END_QUOTE

But the "mechanisms" in the cited papers (noise traders, reporting bias, effort noise) are all /perturbations/ to a fundamentally identified system. Remove the noise trader from GS and price perfectly reveals $\theta$. Remove the reporting bias from Fischer & Verrecchia and the report perfectly reveals earnings. These are 1-to-1 systems with added noise.

The paper under review identifies a /structurally/ unidentified system: even in the total absence of noise, a single price cannot reveal n parameters when n > 1. This is a different and stronger result. The distinction is between:

- *Noisy identification*: The system has a unique solution, but noise prevents its exact recovery. More or better information can always improve the estimate. (GS, Fischer & Verrecchia)
- *Structural non-identification*: The system has infinitely many solutions. No amount of noise reduction can recover the individual parameters from the aggregate signal. (This paper)

This distinction is not made in any of the papers the referee cites.

** Recommended Response

1. Frame the contribution around the /identification/ problem explicitly, distinguishing structural non-identification from noisy identification.
2. Cite and discuss Avery & Zemsky (1998) as the closest related work, explaining how this paper extends their insight.
3. Cite and discuss the papers raised by the referee, explaining why each addresses a different question.
4. Add a formal impossibility result: for n > 1, no updating rule applied to a scalar price signal can uniquely recover the n-dimensional parameter vector.

* Summary Assessment

| Criticism | Fair? | Reason |
|-----------|-------|--------|
| 1. Model incomprehensible | Partially | Presentation needs formal math, but "not anywhere close to GS" is overstated |
| 2. Convergence known (Gode & Sunder) | No | Gode & Sunder studies a different question entirely; incomplete learning is structural, not from agent suboptimality |
| 3. Multi-signal learning known | Partially | The general principle is established (Radner/Jordan), but the referee's specific citations (GS, Fischer & Verrecchia, Feltham & Xie) all have 1:1 parameter-to-signal ratios and address noisy identification, not structural non-identification. The paper's specific contribution --- applying this to single-security investor learning and measuring scaling with $n$ --- is not preempted. |

The paper's core contribution --- that aggregate price efficiency does not imply individual parameter efficiency, and that this is a /structural/ limitation rather than a noise problem --- is not preempted by the cited literature. The primary weakness is presentation: the paper needs formal mathematics, explicit statement of departures from GS, and a clear framing of the identification problem as distinct from the noise problems studied in the existing literature.

* Comprehensive Literature Search (2026-02-06)

To verify the uniqueness of the paper's contribution, we conducted a comprehensive search of the literature across five dimensions: (1) multi-parameter price learning in finance theory, (2) rational expectations and information aggregation, (3) accounting and market efficiency, (4) agent-based market simulations, and (5) signal extraction in econometrics and macroeconomics. The search covered approximately 60 papers published between 1972 and 2026, with particular attention to work published since the original 2019 submission.

** The General Theoretical Result IS Known --- But It Concerns a Different Setting

Three foundational papers in general equilibrium theory establish the dimensionality constraint that underpins the paper's identification problem:

- *Radner (1979)*: "Rational Expectations Equilibrium: Generic Existence and the Information Revealed by Prices." /Econometrica/. Establishes that fully-revealing REE requires dim(price space) $\geq$ dim(state space). When this condition holds with finite states, full revelation is generic.

- *Allen (1982)*: "Strict Rational Expectations Equilibria with Diffuseness." /Journal of Economic Theory/. Explicitly states the necessary condition that the price space must have at least as high a dimension as the state space for generic full revelation.

- *Jordan (1982)*: "The Generic Existence of Rational Expectations Equilibrium in the Higher Dimensional Case." /Journal of Economic Theory/. Proves the converse: when dim(states) > dim(prices), fully revealing REE /generically do not exist/. This is the sharpest impossibility result and the direct theoretical precedent.

- *Pietra & Siconolfi (2008)*: "Trade and Revelation of Information." /Journal of Economic Theory/. Takes the impossibility as given and characterizes the /partially revealing/ equilibria that arise when there are more dimensions of uncertainty than prices.

*** What Radner and Jordan actually show --- and what they do not

It is critical to understand what "prices" and "states" mean in the Radner/Jordan framework. The "prices" in these papers are the /vector of commodity prices across an entire multi-commodity economy/ --- not the price of a single security. In an economy with $K$ commodities, there are $K$ prices, forming a $K$-dimensional price vector. The "states" are states of nature (e.g., whether it rains or shines), not individual valuation parameters of a single asset.

These are /existence results about equilibrium price functions/ in the tradition of Arrow-Debreu general equilibrium. They ask: does there exist a mapping from states of the world to commodity price vectors such that, in equilibrium, every trader can infer the state from observed prices? The answer is: generically yes when dim(prices) $\geq$ dim(states), generically no otherwise.

What Radner and Jordan do /not/ do:
- They do not model /investor learning dynamics/ --- their result is about whether a fully-revealing equilibrium /exists/, not about whether agents can /converge/ to it.
- They do not consider /single securities/ --- their setting is a multi-commodity exchange economy.
- They do not /measure/ the extent of mispricing when the condition fails.
- They do not connect to accounting or finance empirics.

*** The trivial violation for single securities

The key insight for the paper under review is this: *for any single security whose value depends on more than one parameter, the Radner/Jordan dimensionality condition is trivially and always violated.* A single security has one price (dim = 1). If its value depends on $n > 1$ parameters, dim(states) > dim(prices) by definition. The condition fails not as an edge case but as the universal default for real securities.

The finance literature has maintained the Radner/Jordan condition /by construction/: Grossman (1976) and GS (1980) model a single security with a single unknown parameter. Admati (1985) extends to $K$ assets with $K$ unknowns, preserving the 1:1 ratio. No standard model asks what happens to a single security with multiple underlying parameters --- yet this is the empirically relevant case, since any real security depends on many underlying factors.

The accounting and empirical finance literatures have compounded this by /assuming the problem away/: earnings response coefficients, event studies, and market efficiency tests all implicitly assume that investors can learn individual valuation parameters (e.g., accruals vs. cash flows, growth vs. profitability) from aggregate stock price movements. The structural identification problem shows this assumption is unfounded.

These papers establish the abstract mathematical principle. *However*, they are general equilibrium existence/non-existence results that concern multi-commodity economies, not single-security investor learning. They do not simulate markets, measure the extent of parameter-level mispricing, study how mispricing scales with $n$, or connect to the accounting literature's assumptions about investor learning. The paper under review is the first to draw the implication for the single-security investor learning problem.

** Applied Papers That USE the Confounding Structure

A second tier of papers builds models where a single price aggregates multiple unknowns, but each studies a different /consequence/:

| Paper | Unknowns | Signals | Consequence Studied |
|-------|----------|---------|-------------------|
| Lucas (1972) | Real + monetary shock | 1 (local price) | Monetary non-neutrality |
| Avery & Zemsky (1998) | 2--3 dimensions of uncertainty | 1 (asset price) | Herding behavior |
| Sockin & Xiong (2015) | Supply + demand shock | 1 (commodity price) | Feedback amplification |
| Goldstein & Yang (2015) | 2 fundamentals | 1 (stock price) | Information complementarities |
| Goldstein & Yang (2019) | 2 dimensions of uncertainty | 1 (stock price) | Disclosure crowds out /or/ crowds in information acquisition depending on disclosure type |
| Benhabib, Wang & Wen (2015) | Fundamental + sentiment | 1 (aggregate price) | Self-fulfilling equilibria |
| Rondina & Walker (2021) | Multiple shocks | Endogenous signals | Confounding dynamics |
| Adams (2024, WP) | Aggregate signals | Endogenous | States asset prices "necessarily fail" invertibility |
| Banerjee, Davis & Gondhi (2018) | Fundamentals + others' beliefs | 1 (stock price) | Transparency paradox |

*** Key details on the most relevant applied papers

**** Goldstein & Yang (2015): "Information Diversity and Complementarities in Trading and Information Acquisition"

/Journal of Finance/, 70(4), 1723--1765. Explicitly models security value as depending on *two fundamentals*, with different traders informed about different fundamentals. Because price is one-dimensional but the state space is two-dimensional, price cannot fully reveal both fundamentals simultaneously. The focus is on strategic complementarities in information acquisition, not on the impossibility per se or on how component-level errors scale with dimensionality. This is the closest finance theory paper.

**** Sockin & Xiong (2015): "Informational Frictions and Commodity Markets"

/Journal of Finance/, 70(5), 2063--2098. Commodity producers observe a single spot price reflecting *both* global economic fundamentals and supply noise. The feedback effects (producers cannot tell if a low price reflects weak demand or ample supply) generate real consequences. This is precisely a 2-unknowns-from-1-signal model, though applied to commodity markets with a feedback mechanism rather than to equity investor learning.

**** Adams (2024): "Macroeconomic Models with Incomplete Information and Endogenous Signals"

Working paper (University of Florida, December 2024; previously circulated as "Rational Expectations with Endogenous Information"). This paper provides the most rigorous modern treatment of the invertibility problem for endogenous signals.

Adams studies macroeconomic models where agents observe /endogenous/ signals --- signals that are themselves determined in equilibrium (like prices). He introduces two key conditions:

1. *Information Feedback Regularity (IFR)*: A /necessary/ condition for a signal-stable equilibrium to exist. It bounds the norm of the information feedback operator: $\|G\Theta\Xi(B_{A1}L^{-1} + B_{A0})\| < 1$. When this fails, the feedback from information to decisions to signals is explosive, and no stable equilibrium exists.

2. *Sufficient Idiosyncrasy Condition (SIC)*: A /sufficient/ condition for /global uniqueness/. The condition requires that the variance matrix of contemporaneous idiosyncratic shocks, $\Sigma_I = S_{X,0}(I - P_G)S^*_{X,0}$, be invertible. This requires at least as many idiosyncratic shocks as signals. SIC guarantees that all fixed points are signal-stable, and since Theorem 6 proves there can be at most one signal-stable fixed point, SIC implies global uniqueness.

The critical insight: *"Models where the endogenous signals are aggregates --- such as asset prices in the confounding dynamics example --- will necessarily fail this condition"* (p.22). When a signal is a pure aggregate (like a stock price), the projection matrix $P_G$ identifies only aggregate shocks. The matrix $I - P_G$ isolates idiosyncratic dimensions --- but aggregation /zeroes these out/. Therefore $\Sigma_I$ is singular, SIC fails, and uniqueness cannot be guaranteed. In his confounding dynamics example, SIC failure leads to multiple equilibria: both a full-information equilibrium and a "confounding dynamics" equilibrium coexist.

*Important distinction*: Adams's framework concerns the /macroeconomic/ setting --- whether a system of aggregate signals across an economy can identify the underlying state --- not the single-security investor learning problem studied in the paper under review. In Adams's models, the signals are aggregate variables (like asset prices or output) observed across the economy, and the question is whether the system of signals can identify macroeconomic states. The paper under review concerns a different setting: a single security with $n$ valuation parameters and one price.

However, the underlying mathematical mechanism is the same: when a signal is a pure aggregate, it cannot be inverted to recover the individual components that produced it. Adams shows that the stock price of a single security is a pure aggregate signal: $V = \sum \theta_i$ becomes a single number that zeroes out all idiosyncratic component information. This implies the invertibility condition necessarily fails. The paper's simulation demonstrates what that failure looks like in practice --- aggregate convergence coexisting with component-level non-identification, with MSE scaling with $n$.

**** Banerjee, Davis & Gondhi (2018): "When Transparency Improves, Must Prices Reflect Fundamentals Better?"

/Review of Financial Studies/, 31(6), 2377--2414. A single price conflates fundamental information with information about others' beliefs. Increasing transparency about fundamentals can make prices /less/ informative because investors respond by shifting to learning about others' beliefs rather than fundamentals.

**** Chabakauri, Yuan & Zachariadis (2022): "Multi-asset Noisy REE with Contingent Claims"

/Review of Economic Studies/, 89(5), 2445--2490. Studies the /solution/ to the dimensionality problem: with multiple assets (and hence multiple prices), it becomes possible to reveal aggregate shocks. However, only assets in the "replicating portfolio" for the aggregate risk factor are informationally relevant. This /confirms/ the structural non-identification problem for a single asset: one price cannot reveal a multidimensional state.

** Accounting Literature: Empirical Support Without Structural Explanation

Several streams in the accounting literature document empirical phenomena consistent with the structural identification problem, without identifying the structural cause:

*** Accrual anomaly literature

- *Sloan (1996)*: "Do Stock Prices Fully Reflect Information in Accruals and Cash Flows About Future Earnings?" /The Accounting Review/. Demonstrates that stock prices behave as if investors fixate on total earnings and fail to distinguish between the differential persistence of accruals vs. cash flows. This is direct evidence that the market does not learn individual component parameters from aggregate price.

- *Fairfield, Whisenant & Yohn (2003)*: "Accrued Earnings and Growth." /The Accounting Review/. Extended Sloan to broader earnings components; mispricing applies to growth in long-term net operating assets generally.

*** Information bundling literature

- *Ramanan (2015)*: "Promoting Informativeness via Staggered Information Releases." /Review of Accounting Studies/, 20, 537--558. DOI: 10.1007/s11142-014-9307-6. Formally shows that a firm releasing multiple pieces of information simultaneously cannot learn from the market's reaction which piece the market is responding to. Staggered release allows each piece to be separately priced. /This paper directly establishes that aggregate simultaneous pricing prevents learning about individual information components./ It is perhaps the most directly supportive paper in the accounting literature. (Note: previously misattributed to Dye & Sridhar, who are acknowledged for comments.)

- *Ahci, Martens & Sextroh (2022)*: "Information Bundling and Capital Market Feedback." Uses patent grant dates to show that bundling information impedes managerial learning from market feedback. When multiple signals are released simultaneously, the aggregate price response cannot be decomposed into individual component signals.

*** Managerial learning from prices

- *Gelsomin & Hutton (2023)*: "The Learning Hypothesis Revisited: A Discussion of Sani, Shroff and White (2023)." /Journal of Accounting and Economics/, 76, 101644. *This is perhaps the single most directly relevant paper in the accounting literature.*

  Gelsomin and Hutton critically examine the "Learning Hypothesis" --- the premise that managers learn useful, decision-relevant private information by observing outsiders' trading activity as reflected in stock prices. They identify /three key assumptions/ underlying the entire literature:

  1. Outsiders have private, relevant, material information unknown to managers.
  2. Outsiders' trading impounds the /value implications/ of that information into stock prices.
  3. Managers can /extract or infer/ outsiders' private, relevant information from observing stock prices --- and use it to improve their real investment decisions.

  They note that the first two assumptions are well-supported empirically, but /the third is not/: "There is no empirical proxy that directly demonstrates managerial learning... managerial learning from stock prices is simply /inferred or assumed/ from the documented changes in investment sensitivity to price (IS2P)" (p.2).

  Their *ice cube metaphor* is a perfect intuitive statement of the identification problem: "Imagine you place an ice cube in a bowl and leave the bowl on the counter. You can reasonably predict the shape of the ice cube in a couple of hours. [Forward inference is easy.] In contrast, however, now imagine you come across a bowl filled with water. What conclusions can you draw? Was the water previously frozen in the shape of a sphere, cube, or pyramid; one ice cube, multiple shapes, shavings, or crushed; or was the water frozen at all?" (p.2). This is precisely the n-to-1 identification problem: many different ice configurations (parameter vectors) produce the same bowl of water (aggregate price). The forward mapping (parameters → price) is well-defined; the inverse mapping (price → parameters) is not.

  They also invoke *Bond, Edmans & Goldstein (2012)*'s distinction between "forecasting price efficiency" (useful for predicting aggregate cash flows) and "revelatory price efficiency" (useful for informing specific real investment decisions). This maps /exactly/ to the paper's distinction between aggregate price convergence and component-level identification. Forecasting efficiency $\neq$ revelatory efficiency, for the precise structural reason that this paper identifies: one price cannot reveal $n$ parameters.

  Section 4 explicitly states: "There is something highly dissatisfying about the /Learning Hypothesis/... the literature leaves unanswered the /what/ and the /how/ managers learn from observing stock prices that affect their real investment decisions" (p.9).

  /Connection to the paper under review/: Gelsomin & Hutton identify /exactly/ the gap that this paper fills. They ask /why/ managers cannot extract individual information from aggregate stock prices --- the paper answers: because it is structurally impossible. The identification problem means that no amount of sophistication in observing prices can recover the individual components that went into them. Gelsomin & Hutton's unvalidated Assumption 3 is not merely unvalidated --- it is structurally unjustified for any security whose value depends on more than one parameter.

- *Bond, Edmans & Goldstein (2012)*: "The Real Effects of Financial Markets." /Annual Review of Financial Economics/. Notes that price efficiency for forecasting aggregate cash flows does not imply price efficiency for informing about individual investment decisions. Their "forecasting vs. revelatory efficiency" distinction is a key conceptual framework.

- *Gao & Liang (2013)*: "Informational Feedback, Adverse Selection, and Optimal Disclosure Policy." /Journal of Accounting Research/. Disclosure crowds out private information production, reducing what managers can learn from prices.

- *Armstrong, Heinle & Luneva (2024)*: "Financial Information and Diverging Beliefs." /Review of Accounting Studies/. When investors are uncertain about the precision of earnings as a signal, the same announcement can cause beliefs to /diverge/ rather than converge.

*** Investor reliance on price over accounting fundamentals

- *Blankespoor, deHaan, Wertz & Zhu (2019)*: "Why Do Individual Investors Disregard Accounting Information? The Roles of Information Awareness and Acquisition Costs." /Journal of Accounting Research/, 57(1), 53--84. DOI: 10.1111/1475-679X.12248. Exploits the introduction of AP automated ("robo-journalism") earnings articles, which present both earnings news and trailing stock returns side by side in the same article. Despite having earnings information delivered directly to them, individual investors /ignore the earnings content/ and trade based on trailing stock returns. The authors rule out awareness costs (investors clearly see the article) and acquisition costs (the data is right there), attributing the behavior to *integration costs* --- the cognitive difficulty of translating component accounting information into a trading decision.

  /Connection to the paper under review/: Blankespoor et al. observe the behavior predicted by the structural identification problem and attribute it to a behavioral/cognitive friction. The structural non-identification result offers a deeper explanation: investors may /rationally/ prefer the aggregate signal (price/returns) over component information (earnings) because individual components cannot be recovered from the aggregate anyway. The "integration cost" is not purely cognitive --- it reflects the genuine structural impossibility of mapping individual accounting components back to their value implications when the security's value depends on multiple parameters. The rational response to an underdetermined system is to rely on the aggregate and ignore the components, which is exactly what Blankespoor et al. document. This reframes their finding from a behavioral anomaly to a rational response to an information-theoretic constraint.

*** Experimental evidence

- *Davis, Korenok & Lightle (2025)*: "The Effects of Public Disclosures and Information Acquisition on Price Informativeness in a Multi-Attribute Asset Market." /Journal of Behavioral and Experimental Finance/, 47, 101084. DOI: 10.1016/j.jbef.2025.101084. A laboratory experiment with a two-component asset of uncertain value. The paper directly tests and confirms that learning individual components from aggregate prices requires strategic disclosure management --- the aggregate price alone does not spontaneously reveal individual attributes.

** Agent-Based / Computational Literature: A Notable Gap

The entire agent-based simulation literature on information aggregation uses *single-dimensional* fundamentals:

| Paper | Fundamental | Addresses n-from-1? |
|-------|-------------|---------------------|
| Gode & Sunder (1993) | Single private value | No |
| Jamal, Maier & Sunder (2024) | Single unknown state | No |
| Lopez-Lira (2025) | Single asset value (LLM agents) | No |
| Corgnet et al. (2020) | Single unknown state | No |
| Axtell & Farmer (2025, survey) | Various, all 1-D | No |
| LeBaron (2006, survey) | Various, all 1-D | No |

No computational or simulation study was found that models V = f($\theta_1, \ldots, \theta_n$) and asks whether traders can learn the individual $\theta_i$ from observing the single price. This is a genuine gap in the literature.

** Multi-Asset Models: The Standard Resolution

The standard theoretical resolution to the dimensionality problem is to add more prices (assets):

- *Admati (1985)*: $K$ assets, $K$ prices, preserving the 1:1 ratio.
- *Chabakauri, Yuan & Zachariadis (2022)*: Multiple assets including derivatives.
- *Chabakauri (2024, WP)*: Large markets with many assets --- "big data" from many prices.

This /confirms/ that the theoretical literature recognizes the dimensional constraint and resolves it by increasing the number of signals, not by extracting more from a single signal.

** Investor Behaviors Consistent with Structural Non-Identification

A wide range of well-documented investor behaviors --- traditionally attributed to cognitive biases, limited attention, or behavioral frictions --- are consistent with the structural impossibility of decomposing an aggregate price signal into individual valuation components. If the structural non-identification result is taken seriously, many of these behaviors can be reinterpreted not as anomalies requiring behavioral explanation but as /rational responses/ to an information-theoretic constraint.

*** Functional fixation: origins and the behavioral explanation

The concept of "functional fixation" originates in Gestalt psychology (Duncker 1945) and was applied to accounting by *Ijiri, Jaedicke & Knight (1966)*, who argued that decision-makers conditioned to interpret accounting outputs in a particular way will continue to assign the same meaning even when the underlying accounting methods change. *Ashton (1976)* provided the first experimental confirmation: participants did not adjust their decision-making when the costing method changed from variable to absorption costing. *Hand (1990)* extended this to capital markets, showing that stock prices respond to economically meaningless accounting gains/losses from early debt extinguishments as if naive investors are misled by reported income effects.

In this tradition, fixation is understood as a /cognitive/ phenomenon: investors /could/ decompose the signal but /fail to/ due to conditioning or limited processing ability. The structural non-identification result suggests an alternative: for multi-parameter securities, investors /cannot/ decompose the aggregate signal regardless of cognitive sophistication, because the system is mathematically underdetermined.

*** The accrual anomaly as component decomposition failure

The most direct evidence of functional fixation in capital markets comes from the accrual anomaly literature:

- *Sloan (1996)*: Stock prices behave as if investors fixate on total earnings without distinguishing between the differential persistence of accruals vs. cash flows. A long-short strategy based on accrual levels earns approximately 12% annually.

- *Xie (2001)*: Extended Sloan by decomposing total accruals into normal and abnormal (discretionary) components. The anomaly is driven primarily by discretionary accruals, suggesting investors fail to distinguish sub-components /within/ accruals as well --- a nested decomposition failure.

- *Bradshaw, Richardson & Sloan (2001)*: Neither sell-side analysts nor auditors incorporate the predictable future earnings declines associated with high accruals into their forecasts. This extends the decomposition failure from retail investors to /professional intermediaries/, suggesting the problem is not merely naivete.

- *Jegadeesh & Livnat (2006)*: Revenue surprises predict future returns beyond what is captured by earnings surprises alone. The individual components of earnings (revenue vs. expense) are not separately priced, consistent with investors observing only the aggregate and failing to decompose it.

The standard explanation is behavioral: investors are cognitively lazy or biased. The structural alternative: total earnings is a single aggregate signal, and decomposing it into accruals, cash flows, revenue, and expense components is an identification problem analogous to the n-parameters-from-1-price problem. Even a fully rational investor observing only the aggregate cannot uniquely determine the components.

*** Post-earnings-announcement drift as a decomposition problem

Post-earnings-announcement drift (PEAD) --- the tendency for cumulative abnormal returns to continue drifting in the direction of the earnings surprise for 60+ trading days --- is one of the most robust anomalies in finance.

- *Bernard & Thomas (1989, 1990)*: Documented the drift and showed that stock prices partially reflect a naive seasonal random walk model of earnings, as if investors expect earnings similar to the same quarter of the prior year without fully processing the time-series properties.

- *Barberis, Shleifer & Vishny (1998)*: Modeled PEAD as arising from "conservatism" --- slow updating of beliefs in response to new evidence. Their model produces both underreaction (to single announcements) and overreaction (to series of same-sign news).

The standard interpretation of PEAD is that investors underreact to earnings news. But the structural non-identification framework offers a different reading: investors may be uncertain about /what portion of the earnings surprise was already reflected in the pre-announcement price/. If the price signal cannot be decomposed into components, then an investor observing a positive earnings surprise cannot easily determine whether the surprise reflects genuinely new information or information that should already have been priced. The rational response to this uncertainty is to revise gradually --- producing drift.

*** Investor reliance on price over fundamentals

- *Blankespoor, deHaan, Wertz & Zhu (2019)*: When AP automated earnings articles present both earnings news and trailing stock returns side by side, individual investors ignore the earnings content and trade based on trailing returns. The authors attribute this to "integration costs."

- *Jegadeesh & Titman (1993)*: Strategies buying past winners and selling past losers generate significant positive returns over 3-to-12-month holding periods. Momentum trading is fundamentally a strategy of relying on the aggregate price signal rather than decomposing fundamentals.

- *Hong & Stein (1999)*: In their model, "newswatchers" observe private fundamental signals but /cannot extract other newswatchers' information from prices/. Information diffuses gradually because agents cannot fully invert the price signal. The inability to extract multidimensional information from a single price directly causes underreaction, and momentum trading is a rational response.

- *Barber & Odean (2008)*: Individual investors are net buyers of attention-grabbing stocks --- those in the news, experiencing extreme returns, or high abnormal volume. Investors substitute aggregate-level price signals for fundamental component analysis.

- *De Bondt & Thaler (1985, 1987)*: Past losers outperform past winners over 3--5 year horizons (long-term reversal). The combined pattern --- intermediate-term momentum followed by long-term reversal --- is consistent with investors initially relying on aggregate price trends and subsequently correcting when component information eventually resolves through other channels.

The structural reinterpretation: if investors cannot decompose price into components, then the most informative signal available /is/ the aggregate --- past returns. Momentum trading and return-chasing are not irrational; they are optimal given the information constraint. Blankespoor et al.'s "integration costs" are not purely cognitive --- they reflect the structural impossibility of mapping individual accounting components to their value implications when the security's value depends on multiple parameters.

*** Limited attention and rational inattention

A parallel literature models limited processing capacity as a rational constraint rather than a cognitive bias:

- *Sims (2003)*: Agents face a Shannon information-theoretic capacity constraint. The "noise" agents experience is endogenous to their optimal attention allocation, not exogenous as in Grossman-Stiglitz.

- *Peng & Xiong (2006)*: When attention is scarce, investors optimally allocate more capacity to market- and sector-level information than to firm-specific information ("category learning"). This is a direct prediction of the structural problem: when decomposing aggregate signals into components is impossible, the rational response is to process aggregates.

- *Hirshleifer & Teoh (2003)*: Formalized limited attention in the disclosure context. When investors have bounded processing capacity, the /presentation format/ of information --- not just its content --- affects market prices. Different levels of aggregation produce different pricing outcomes. If investors could perfectly decompose aggregates, presentation format would be irrelevant.

- *Hirshleifer, Lim & Teoh (2009)*: The immediate price reaction to a firm's earnings surprise is weaker, and post-announcement drift stronger, when more firms announce earnings on the same day ("distraction"). When investors must divide attention across multiple signals, their ability to process any individual firm's components deteriorates.

- *DellaVigna & Pollet (2009)*: Friday earnings announcements produce 15% weaker immediate responses and 70% larger post-announcement drift. The degree of attention investors allocate directly affects how well they can process component information.

- *Blankespoor, deHaan & Marinovic (2020)*: Propose a three-stage framework for disclosure processing costs: (1) awareness costs, (2) acquisition costs, and (3) integration costs. Even when the first two are eliminated, the third --- "integration" --- remains binding. This is precisely the structural constraint: investors can freely observe price (awareness) and read disclosures (acquisition), but /integrating/ component information into a valuation when the aggregate signal is structurally underdetermined is where the impossibility bites.

The rational inattention literature provides the theoretical complement to the structural result. Sims and followers show that finite capacity makes investors optimally allocate attention to aggregates; the structural non-identification result shows that even /infinite/ capacity would not solve the decomposition problem for a single price signal. The two mechanisms reinforce each other: structural non-identification makes component-level processing literally impossible from price alone, and limited attention makes it costly to acquire the supplementary signals that could help.

*** Stale information reactions

- *Tetlock (2011)*: Firms' stock returns respond to stale news (recycled stories textually similar to recent coverage), and this response partially reverses within a week. Individual investors trade /more aggressively/ on stale news. The standard interpretation is that investors cannot distinguish old from new information.

- *Huberman & Regev (2001)*: EntreMed's stock jumped from $12 to $85 after a /New York Times/ article about cancer drug research that had been published in /Nature/ five months earlier. Investors could not determine from the aggregate price that this information was already incorporated.

The structural reinterpretation: investors responding to stale information may not be "confused" --- they may be rationally responding to /any/ signal that helps them solve the decomposition problem. If price alone cannot tell investors /which/ components are reflected in it, then a news article that identifies a specific component (even if the article is old) provides genuinely useful information: it helps investors attribute portions of the aggregate price to specific fundamentals. The problem is not that investors fail to recognize staleness; it is that they cannot determine from price alone what is already priced.

*** Investor disagreement as a consequence of non-identification

- *Kandel & Pearson (1995)*: Trading volume at earnings announcements cannot be explained without allowing for differential interpretation of the same public signal. Investors receiving identical information decompose it differently.

- *Hong & Stein (2007)*: Advocate for "disagreement models" where heterogeneous beliefs drive prices and volume. When investors "agree to disagree," they do not infer that disagreement implies informational disadvantage.

The structural connection: if a single price cannot reveal $n$ parameters, then investors with different priors about those parameters will /rationally and genuinely/ disagree even when observing the same price. The solution set is an $(n-1)$-dimensional hyperplane, and each investor's posterior will lie at a different point on that hyperplane depending on their prior. Disagreement is not irrational --- it is the inevitable consequence of structural non-identification. This provides a new foundation for disagreement models: the heterogeneity is not ad hoc but arises naturally from the dimensionality mismatch.

*** The structural vs. behavioral debate

Two papers frame the central question:

- *Bloomfield (2002)*: The "Incomplete Revelation Hypothesis" (IRH) proposes that statistics more costly to extract from public data are less completely revealed in market prices. This reframes functional fixation from an irrational bias to a rational response to extraction costs. The IRH predicts the same patterns as the fixation hypothesis but grounds them in rational economics. However, Bloomfield treats the cost as /finite/ --- a matter of degree. The structural non-identification result goes further: for a single price signal with $n > 1$ parameters, the cost is /infinite/ because the decomposition is impossible, not merely expensive.

- *Brav & Heaton (2002)*: Demonstrate that behavioral models (investor irrationality) and rational structural uncertainty models (incomplete information about the economic environment) produce /identical predictions/ for underreaction and overreaction. The rational model, though fully Bayesian, embodies heavy weighting of prior opinion and recent data --- the same features attributed to behavioral biases. This is precisely the paper's point: what the behavioral literature calls "fixation" or "underreaction" may be the rational consequence of structural constraints on what can be learned from price.

The paper under review contributes to this debate by identifying a /specific/ structural constraint --- the dimensionality mismatch between the parameter space and the signal space --- that produces behaviors observationally equivalent to functional fixation, limited attention, underreaction, return-chasing, and disagreement. If the constraint is structural, then the behavioral explanations are not wrong (investors /do/ fixate, /do/ underreact, /do/ chase returns) but are incomplete: they describe the surface behavior without identifying the information-theoretic root cause.

** Competing Explanations in the Empirical Literature

The empirical papers documenting complexity-related anomalies offer explanations that fall into several camps. Critically, /none/ of them identify the structural non-identification mechanism. Understanding what each paper claims --- and what it does not claim --- is essential for positioning the paper's contribution.

*** Camp 1: Information processing costs (the problem is "hard")

*Barinov, Park & Yildizhan (2024)*, "Firm Complexity and Post-Earnings-Announcement Drift" (/Review of Accounting Studies/, 29(1), 527--579), provide the most striking empirical evidence: PEAD is *2x larger* for existing conglomerates and *8x larger* for new conglomerates (those that organically create new business lines). Their explanation: conglomerates have "more costly and difficult" information environments. Evidence: fewer analysts, less expert analysts, larger forecast errors, lower institutional ownership.

*Cohen & Lou (2012)*, "Complicated Firms" (/Journal of Financial Economics/, 104(2), 383--400), show that returns of "pseudo-conglomerates" (portfolios of standalone firms matching a conglomerate's segment composition) predict the actual conglomerate's return next month. A trading strategy exploiting this yields *118 bps per month*. Their explanation: when industry news arrives, simple firms update quickly but conglomerates update with a lag because investors have "limited resources and capacity to collect, interpret, and trade on value-relevant information."

Both papers explicitly reject limits-to-arbitrage as the primary explanation (conglomerates are larger, not smaller, so traditional arbitrage barriers cannot explain the effect). Both invoke the Grossman-Stiglitz tradition of costly information processing.

*What they do not consider*: Both treat the problem as /quantitatively/ harder for complex firms --- more industries to analyze, higher costs per analyst. Neither considers that the problem may be /qualitatively/ impossible: a single stock price is dimensionally insufficient to convey $n$ independent segment fundamentals regardless of analyst sophistication or investor attention.

*** Camp 2: Gradual information diffusion (the problem is "slow")

*Hong, Lim & Stein (2000)*, "Bad News Travels Slowly" (/Journal of Finance/, 55(1), 265--295), attribute momentum to gradual information diffusion. In the Hong & Stein (1999) model, "newswatchers" observe private signals but cannot extract others' information from prices. Information reaches the investing public slowly, and the fewer analysts covering a stock, the slower the diffusion. The effect is asymmetric: much stronger for bad news (managers proactively publicize good news but not bad news).

They explicitly test against and reject:
- Daniel, Hirshleifer & Subrahmanyam (1998) overconfidence model (which does not predict coverage-dependent momentum)
- Barberis, Shleifer & Vishny (1998) conservatism/representativeness model (which does not generate cross-sectional predictions tied to coverage)
- Risk-based explanations (momentum is not compensation for risk)

*What they do not consider*: Analysts are modeled as /conduits/ that accelerate the rate at which existing information reaches investors --- not as providers of additional signal dimensions. The mechanism is about speed, not about solving a dimensional identification problem. Hong & Stein (1999) /assume/ that price is non-invertible (newswatchers "cannot extract" others' information from prices) but do not explain /why/ --- they treat it as a behavioral assumption. The structural non-identification result provides the microfoundation: investors cannot invert the price signal because dim(states) > dim(prices).

*** Camp 3: Additional signals resolve ambiguity (the problem is "solvable with more data")

*Mohanram (2014)*, "Analysts' Cash Flow Forecasts and the Decline of the Accruals Anomaly" (/Contemporary Accounting Research/, 31(4), 1143--1170), shows that analyst cash flow forecasts /implicitly decompose/ earnings into cash flow and accrual components, helping investors assess differential persistence. The accrual anomaly weakens significantly when cash flow forecasts are available. He tests against and survives: hedge fund arbitrage (Green, Hand & Soliman 2011), improved accrual quality, increased market liquidity (Chordia, Subrahmanyam & Tong 2014).

*Li, Ramesh & Shen (2021)*, "Does Voluntary Balance Sheet Disclosure Mitigate Post-Earnings-Announcement Drift?" (/Journal of Accounting and Public Policy/, 40(1)), show that voluntary balance sheet disclosure reduces PEAD through two channels: helping investors assess earnings persistence and reducing information processing costs. The effect is stronger when earnings uncertainty is higher.

*Ettredge, Kwon, Smith & Zarowin (2005)*, "The Impact of SFAS No. 131 Business Segment Data on the Market's Ability to Anticipate Future Earnings" (/The Accounting Review/, 80(3), 773--804), show that SFAS 131 improved FERC (forward earnings response coefficients) for affected firms. Segment data helped investors forecast /aggregate/ future earnings better.

*Park (2011)*, "The Effect of SFAS 131 on the Stock Market's Ability to Predict Industry-wide and Firm-specific Components of Future Earnings" (/Accounting & Finance/, 51(2), 575--607), provides the most striking finding for the structural thesis. He decomposes future earnings into industry-wide and firm-specific components and finds that SFAS 131 improved pricing of the *industry-wide component* but *NOT the firm-specific component*. His explanation: segment data reveals which industries a firm operates in but does not help predict firm-specific performance. He also suggests that the loss of geographic segment data under SFAS 131 offset some gains.

*Connection to structural non-identification*: Park's asymmetric finding is /exactly/ what the structural model predicts. Knowing which segments exist tells investors the "weights" in the sum-of-parts --- it reveals the dimensionality of the problem and helps with industry-level factors that are common across firms. But it does not resolve firm-specific components, which require firm-specific signals. Park does not frame his result in these terms, but the pattern is a direct empirical confirmation of dimensional collapse: segment disclosure collapses the /industry/ dimension but leaves the /firm-specific/ dimension unresolved.

*** Camp 4: Behavioral fixation (the problem is "biased")

*Shi & Zhang (2012)*, "Can the Earnings Fixation Hypothesis Explain the Accrual Anomaly?" (/Review of Accounting Studies/, 17(1), 1--21), derive from a formal analytical model that the profitability of the accrual trading strategy is the product of (a) the earnings response coefficient (how much investors react to aggregate earnings) and (b) the persistence gap between cash flows and accruals (how costly the fixation error is). This is the most rigorous theoretical framework in the accrual anomaly literature.

*What they do not consider*: Their model treats fixation as /behavioral/ --- investors /could/ decompose earnings into cash flows and accruals but /fail to/. The structural alternative: for an investor observing only aggregate price, the decomposition is impossible regardless of cognitive sophistication. Shi & Zhang's persistence gap is a special case of the differential persistence framework with $n = 2$ (cash flows and accruals), but they derive the prediction from behavioral assumptions rather than from the structural impossibility of inverting the price signal.

*** Summary: the key distinction

| Explanation | Representative papers | The problem is... | Solvable with more resources? |
|---|---|---|---|
| Processing costs | Barinov et al. (2024), Cohen & Lou (2012) | Hard | Yes (better analysts, more resources) |
| Gradual diffusion | Hong, Lim & Stein (2000) | Slow | Yes (faster intermediaries) |
| Limited attention | Peng & Xiong (2006), Hirshleifer & Teoh (2003) | Neglected | Yes (more attention) |
| Behavioral fixation | Shi & Zhang (2012), Hand (1990) | Biased | Yes (debiasing) |
| Additional signals | Mohanram (2014), Li et al. (2021), Ettredge et al. (2005) | Incomplete | Yes (more signals) |
| *Structural non-identification* | *This paper* | *Impossible* | *No* |

Every existing explanation treats the problem as /difficult but in principle solvable/: with better analysts, more attention, faster diffusion, less bias, or more signals, investors could fully price individual components. The structural non-identification result says something qualitatively different: the problem is /mathematically impossible/ from price alone because $\dim(\text{states}) > \dim(\text{prices})$.

This distinction has three important implications:

1. *Reinterpretation*: What existing papers call "processing costs" (Barinov et al., Cohen & Lou), "integration costs" (Blankespoor et al.), or "limited attention" (Hirshleifer & Teoh) may actually be the /rational response/ to structural impossibility. Investors do not decompose conglomerate earnings because they /cannot/, not because they are cognitively limited. The structural explanation is observationally consistent with all documented patterns but provides a more fundamental mechanism.

2. *Park (2011) as quasi-empirical test*: Park's finding that disaggregation helps with industry-wide components but not firm-specific components is difficult to explain under processing-cost or attention stories (why would attention or processing costs differ between industry-wide and firm-specific components?). Under structural non-identification, it is predicted: segment disclosure reveals the industry dimension but leaves the firm-specific dimension unresolved because firm-specific persistence rates require firm-specific signals.

3. *Untested predictions*: The structural framework generates predictions that existing explanations do not:
   - The /dispersion/ of persistence rates across $n > 2$ components should predict anomaly magnitude (extends Shi & Zhang from $n = 2$)
   - The number of segments/parameters $n$ should predict anomaly strength directly (not merely as a proxy for processing difficulty)
   - Sequential signal collection should exhibit diminishing marginal value as dimensions collapse

** Three Types of Signals and the Reinterpretation of Private Information

The discussion above focused on /component observations/ --- direct observations of $\theta_i$ that collapse one dimension of the solution set. But the identification framework reveals two additional types of information that constrain the solution set:

*** Component observations

Direct observations of parameter values: $\theta_i = c$. An earnings disclosure, an analyst cash flow estimate, or a segment report provides this type of signal. Each eliminates one unknown, collapsing the solution set by one dimension.

*** Structural observations

Observations of the /mapping/ from components to price, rather than of a component value. The earnings response coefficient (ERC) is the canonical example. When an investor observes both an earnings surprise $\Delta\theta_{\text{earnings}}$ and the resulting price response $\Delta V$, the ratio $\Delta V / \Delta\theta_{\text{earnings}}$ estimates the partial derivative $\partial V / \partial \theta_{\text{earnings}}$. This constrains the /shape/ of the solution set without pinning down a specific $\theta_i$. ERCs less than one indicate that other components moved in the opposite direction, or that the market had already partially priced the earnings news, or that earnings have implications for other components.

Every ERC regression, FERC analysis, and measure of "price informativeness" in the empirical literature is, in this framework, an estimate of a structural parameter of the identification problem. This gives the ERC literature a new interpretation: it is not merely measuring "how efficiently the market prices earnings" but providing partial solutions to the dimensional decomposition problem.

*** Relative observations

Comparisons of price responses across events. If the market responds strongly to an earnings surprise but weakly to a revenue surprise, this reveals which dimensions the market had already priced and which remain unresolved. Each event-study observation is informative not just about news content but about the /residual structure/ of the identification problem.

*** Reinterpretation of private information

The standard assumption is that "private information" means a component observation --- an insider knows $\theta_i$ before the market. The framework reveals that private information can take any of the three forms above:

- *Component knowledge*: A manager who knows her firm's earnings, costs, and investment plans has direct component observations.
- *Structural knowledge*: A manager who knows how components combine and persist --- which are transitory, which are permanent, how changes in one component affect others --- has structural observations that constrain the solution set.
- *Relative knowledge*: An analyst who has observed how this firm's price responds to different types of news has relative observations that help decompose future price movements.

This creates a hierarchy of decomposition ability:
1. *Managers*: extensive component + structural knowledge → can decompose many dimensions of price
2. *Specialized analysts*: industry structural knowledge + some component estimates → can decompose several dimensions
3. *Institutional investors*: proprietary structural models (factor loadings, persistence estimates) → can decompose some dimensions
4. *Retail investors*: price only → effectively no decomposition ability

This hierarchy maps to empirically observed patterns in forecast accuracy, trading performance, and the value of analyst coverage. It also provides a more precise answer to citet:gelsominLearningHypothesisRevisited2023's question about managerial learning from price: managers /can/ learn from price, but only about dimensions their supplementary signals allow them to isolate. Assumption 3 is conditionally true, and the structural framework specifies the boundary.

*** Connection to the "processing costs" literature

This typology reframes what "processing costs" actually are. In the existing literature (Grossman & Stiglitz: acquisition costs; Bloomfield 2002: extraction costs; Blankespoor et al. 2020: integration costs), processing costs are a generic friction --- a scalar that makes pricing less efficient but could in principle be overcome. The structural framework gives these costs specific content: the "cost" of processing a signal depends on /which type of signal it is/ and /which dimension of the identification problem it addresses/. For component observations, the cost is the cost of acquiring the disclosure. For structural observations, the cost is the cost of estimating the mapping (e.g., running an ERC regression). For dimensions where no signal exists, the "cost" is infinite --- not because processing is hard, but because the required information is not in the available signal set.

This unifies the "informativeness of price" literature and the "value of disclosure" literature, which are usually treated as separate conversations. Price is informationally complete along one dimension (the aggregate) and informationally empty along the remaining $n-1$ orthogonal dimensions. Disclosure is valuable because each disclosure resolves a specific dimension that price cannot. The value of any particular disclosure depends on /which/ dimension it addresses relative to what the existing signal set already conveys --- not on a generic "more information is better" principle. This provides a more disciplined framework than "processing costs are high" or "price is informative" deployed selectively.

** Synthesis: What Is Unique and What Must Be Cited

*** The abstract mathematical principle is established --- in a different context (cite and acknowledge)

Jordan (1982) / Allen (1982) / Radner (1979) establish the dimensional impossibility in abstract multi-commodity general equilibrium. The paper must cite these and position its contribution relative to them. However, the relationship requires careful articulation:

- Radner and Jordan prove that /equilibrium price functions/ in multi-commodity economies cannot generically be fully revealing when dim(states) > dim(prices). Their "prices" are commodity price vectors, not single stock prices. Their results concern /existence of equilibria/, not /investor learning dynamics/.
- The paper under review shows that /for any single security with multiple underlying parameters/, the condition is trivially and always violated --- one price, $n > 1$ unknowns. This is not a special case of Radner/Jordan (which concerns multi-commodity economies) but rather a parallel application of the same mathematical principle (injectivity requires sufficient dimensionality) to a fundamentally different setting (single-security investor learning).
- The contribution is /not/ the mathematical principle itself, but its implication for a setting the theoretical literature has not examined and the empirical literature has assumed away.

*** The specific contribution IS unique

No paper found:

1. *Draws the implication* of the dimensionality constraint for the /single-security investor learning problem/. The Radner/Jordan condition is trivially and always violated for any real security, yet neither the theoretical finance literature (which maintains the condition by construction) nor the empirical accounting literature (which assumes the problem away) has noted this.
2. *Simulates* a market where $V = \sum \theta_i$ and *measures* component-level learning failure as a function of $n$.
3. *Connects* the structural identification problem to the *accounting literature's* implicit assumption that investors learn individual valuation parameters from price.
4. *Demonstrates* that aggregate price convergence *coexists* with component-level non-identification --- the market gets the sum right while being structurally unable to recover the parts.
5. *Frames* the absence of noise traders as a feature: structural non-identification replaces exogenous noise as the barrier to perfect learning. This insight is implicit in the theoretical literature but is never stated or explored.
6. *Measures* how MSE scales with the number of parameters $n$.

*** Recommended positioning for the revised paper

The paper should:

1. Cite Jordan (1982), Allen (1982), and Radner (1979), carefully distinguishing what they show (existence/non-existence of fully-revealing equilibria in multi-commodity economies) from what the paper shows (investor learning dynamics for a single security with multiple parameters). Emphasize that the Radner/Jordan condition is trivially violated for any single security with $n > 1$ parameters, yet neither the theoretical nor empirical literature has drawn this implication.

2. Cite Goldstein & Yang (2015) as the closest finance theory paper, noting that their model uses $n = 2$ fundamentals and one price but focuses on information complementarities, not on measuring component-level learning failure or connecting to accounting assumptions.

3. Cite Sockin & Xiong (2015) and Avery & Zemsky (1998) as applied examples that exploit the confounding structure for different purposes (commodity feedback and herding, respectively).

4. Cite Adams (2024) for the explicit statement that aggregate signals fail the invertibility condition. Note that Adams's framework concerns the macroeconomic setting (whether a system of aggregate signals across an economy can identify the underlying state) rather than the single-security problem, but the underlying mathematical mechanism is the same.

5. Cite Ramanan (2015) and the accrual anomaly literature (Sloan 1996) as accounting evidence that is /consistent with/ the structural explanation but has not been connected to it.

6. Position the simulation as filling a gap: the entire ABM literature uses single-dimensional fundamentals, and no prior study demonstrates the coexistence of aggregate convergence with component-level non-identification.

* Potential Additional References

*** Previously identified (from initial review analysis)

- *Avery & Zemsky (1998)*: "Multidimensional Uncertainty and Herd Behavior in Financial Markets." /American Economic Review/. Closest existing work on multi-dimensional identification failure from a single price.
- *Gode & Sunder (1993)*: Should be cited and distinguished, since the referee raised it.
- *Blume & Easley (1982)*: Explicitly states that signals and prices must be paired for Bayesian learning --- supports the paper's argument that the literature assumes the problem away.
- *Admati (1985)*: Multi-asset REE with K assets and K prices. Shows that the literature preserves the 1:1 ratio even when extending to multiple assets.
- *Hong & Stein (2007)*: "Disagreement and the Stock Market." Allows investors to observe only one of two signals. Relevant to partial information models.
- *Romer (1993)*: "Rational Asset-Price Movements Without News." Prices can move because trading reveals information imperfectly aggregated from multiple dimensions --- relevant but frames the result as about price movements, not learning.

*** Newly identified (from comprehensive literature search)

**** Must cite (directly relevant to positioning)

- *Jordan (1982)*: "The Generic Existence of Rational Expectations Equilibrium in the Higher Dimensional Case." /Journal of Economic Theory/. The sharpest impossibility result: when dim(states) > dim(prices), fully revealing REE generically do not exist.
- *Allen (1982)*: "Strict Rational Expectations Equilibria with Diffuseness." /Journal of Economic Theory/. States the dimensional necessary condition.
- *Radner (1979)*: "Rational Expectations Equilibrium: Generic Existence." /Econometrica/. Establishes that dim(prices) $\geq$ dim(states) is needed for generic full revelation.
- *Goldstein & Yang (2015)*: "Information Diversity and Complementarities in Trading and Information Acquisition." /Journal of Finance/. Closest finance theory paper: 2 fundamentals, 1 price.
- *Sockin & Xiong (2015)*: "Informational Frictions and Commodity Markets." /Journal of Finance/. Clean 2-unknowns-from-1-price applied model.
- *Adams (2024)*: "Macroeconomic Models with Incomplete Information and Endogenous Signals." Working paper (University of Florida). Introduces Information Feedback Regularity and Sufficient Idiosyncrasy Condition; proves aggregate signals (like asset prices) necessarily fail invertibility.
- *Ramanan (2015)*: "Promoting Informativeness via Staggered Information Releases." /Review of Accounting Studies/, 20, 537--558. Formally shows bundled signals prevent component-level learning.

- *Gelsomin & Hutton (2023)*: "The Learning Hypothesis Revisited." /Journal of Accounting and Economics/, 76, 101644. Explicitly questions whether managers can extract individual information from aggregate stock prices; ice cube metaphor perfectly illustrates the identification problem; identifies the unvalidated assumption that this paper shows is structurally unjustified.

**** Should cite (strongly supportive or closely related)

- *Goldstein & Yang (2019)*: "Good Disclosure, Bad Disclosure." /Journal of Financial Economics/. Disclosure about one dimension can crowd out /or/ crowd in information acquisition about another, depending on the type of information disclosed.
- *Sloan (1996)*: "Do Stock Prices Fully Reflect Information in Accruals and Cash Flows?" /The Accounting Review/. Empirical evidence consistent with the structural explanation.
- *Benhabib, Wang & Wen (2015)*: "Sentiments and Aggregate Demand Fluctuations." /Econometrica/. Price conflates fundamental and sentiment; signal extraction cannot separate them.
- *Rondina & Walker (2021)*: "Confounding Dynamics." /Journal of Economic Theory/. Dynamic version of the confounding problem.
- *Banerjee, Davis & Gondhi (2018)*: "When Transparency Improves, Must Prices Reflect Fundamentals Better?" /Review of Financial Studies/. A single price conflates fundamentals with information about others' beliefs.
- *Pietra & Siconolfi (2008)*: "Trade and Revelation of Information." /Journal of Economic Theory/. Characterizes partial revelation when dim(uncertainty) > dim(prices).
- *Chabakauri, Yuan & Zachariadis (2022)*: "Multi-asset Noisy REE with Contingent Claims." /Review of Economic Studies/. Shows multiple prices needed to resolve the dimensional constraint.
- *Bond, Edmans & Goldstein (2012)*: "The Real Effects of Financial Markets." /Annual Review of Financial Economics/. Aggregate price efficiency $\neq$ component efficiency.

**** Consider citing (contextually relevant)

- *Lucas (1972)*: The original confounding model (2 shocks, 1 price).
- *Ahci, Martens & Sextroh (2022)*: Experimental evidence that information bundling impedes learning.
- *Armstrong, Heinle & Luneva (2024)*: Precision uncertainty causes belief divergence.
- *Jamal, Maier & Sunder (2024)*: Recent ABM work (cite to show the gap in the computational literature).
- *Axtell & Farmer (2025)*: ABM survey (cite to show the gap).
- *Lou, Parsa & Ray (2019) / Rostek & Weretka (2019)*: Multidimensional signals but 1-D fundamental --- shows the distinction between signal dimensionality and fundamental dimensionality.
- *Vives (2008)*: /Information and Learning in Markets/ (textbook). Comprehensive reference on information aggregation.
- *Siga & Mihm (2021)*: "Information Aggregation in Competitive Markets." /Theoretical Economics/. Studies multidimensional states with a single price as aggregation mechanism.
